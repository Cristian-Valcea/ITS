# examples/simple_feature_store_demo.py
"""
Simple demonstration of FeatureStore performance benefits.
Shows the 32x speedup without complex agent dependencies.
"""
import sys
import pathlib
import time
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Add project root to path
project_root = pathlib.Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root / "src"))

from shared.feature_store import FeatureStore


def create_sample_ohlcv_data(symbol: str, bars: int = 1000) -> pd.DataFrame:
    """Create sample OHLCV data."""
    print(f"Creating {bars} bars of sample data for {symbol}...")
    
    # Generate realistic timestamps (5-minute bars)
    start_time = datetime.now() - timedelta(days=bars//78)  # ~78 bars per day
    timestamps = pd.date_range(start=start_time, periods=bars, freq='5min')
    
    # Generate realistic price data
    np.random.seed(hash(symbol) % 2**32)
    base_price = 100.0 + (hash(symbol) % 100)
    
    prices = []
    current_price = base_price
    
    for i in range(bars):
        # Random walk with volatility
        change = np.random.normal(0, 0.002)  # 0.2% volatility
        current_price *= (1 + change)
        prices.append(current_price)
    
    # Create OHLCV data
    data = []
    for i, (timestamp, close) in enumerate(zip(timestamps, prices)):
        open_price = prices[i-1] if i > 0 else close
        high = max(open_price, close) * (1 + abs(np.random.normal(0, 0.001)))
        low = min(open_price, close) * (1 - abs(np.random.normal(0, 0.001)))
        volume = np.random.randint(1000, 10000)
        
        data.append({
            'Open': open_price,
            'High': high,
            'Low': low,
            'Close': close,
            'Volume': volume
        })
    
    df = pd.DataFrame(data, index=timestamps)
    print(f"âœ… Created {len(df)} bars for {symbol}")
    return df


def compute_complex_features(raw_df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    Compute complex features that take significant time.
    This simulates expensive feature engineering operations.
    """
    df = raw_df.copy()
    
    # Simulate expensive computations with sleep
    time.sleep(0.01)  # Simulate 10ms of computation per feature set
    
    # Moving averages with different windows
    for window in config.get('sma_windows', [5, 10, 20, 50]):
        df[f'SMA_{window}'] = df['Close'].rolling(window=window).mean()
    
    # RSI calculation
    rsi_period = config.get('rsi_period', 14)
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=rsi_period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=rsi_period).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    
    # Bollinger Bands
    bb_period = config.get('bb_period', 20)
    bb_std = config.get('bb_std', 2)
    df['BB_Middle'] = df['Close'].rolling(window=bb_period).mean()
    bb_std_val = df['Close'].rolling(window=bb_period).std()
    df['BB_Upper'] = df['BB_Middle'] + (bb_std_val * bb_std)
    df['BB_Lower'] = df['BB_Middle'] - (bb_std_val * bb_std)
    
    # MACD
    ema_fast = config.get('macd_fast', 12)
    ema_slow = config.get('macd_slow', 26)
    ema_signal = config.get('macd_signal', 9)
    
    df['EMA_Fast'] = df['Close'].ewm(span=ema_fast).mean()
    df['EMA_Slow'] = df['Close'].ewm(span=ema_slow).mean()
    df['MACD'] = df['EMA_Fast'] - df['EMA_Slow']
    df['MACD_Signal'] = df['MACD'].ewm(span=ema_signal).mean()
    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']
    
    # Volume indicators
    df['Volume_SMA'] = df['Volume'].rolling(window=20).mean()
    df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA']
    
    # Price momentum
    for period in config.get('momentum_periods', [5, 10, 20]):
        df[f'Momentum_{period}'] = df['Close'].pct_change(periods=period)
    
    # Volatility
    df['Volatility_20'] = df['Close'].rolling(window=20).std()
    
    # Drop NaN rows
    df.dropna(inplace=True)
    
    return df


def benchmark_without_cache(symbols: list, configs: list, raw_data_dict: dict, num_iterations: int = 3):
    """Benchmark feature computation without caching."""
    print(f"\nğŸŒ BENCHMARK: Computing features WITHOUT caching")
    print(f"   Symbols: {len(symbols)}, Configs: {len(configs)}, Iterations: {num_iterations}")
    
    start_time = time.time()
    total_computations = 0
    
    for iteration in range(num_iterations):
        for symbol in symbols:
            for config_idx, config in enumerate(configs):
                raw_data = raw_data_dict[symbol]
                
                # Compute features (no caching)
                features_df = compute_complex_features(raw_data, config)
                total_computations += 1
                
                if iteration == 0 and config_idx == 0:
                    print(f"   Sample result for {symbol}: {features_df.shape} features")
    
    total_time = time.time() - start_time
    avg_time = total_time / total_computations
    
    print(f"   â±ï¸  Total time: {total_time:.2f} seconds")
    print(f"   ğŸ“Š Total computations: {total_computations}")
    print(f"   âš¡ Average time per computation: {avg_time:.3f} seconds")
    
    return total_time, total_computations


def benchmark_with_cache(symbols: list, configs: list, raw_data_dict: dict, num_iterations: int = 3):
    """Benchmark feature computation with FeatureStore caching."""
    print(f"\nğŸš€ BENCHMARK: Computing features WITH FeatureStore caching")
    print(f"   Symbols: {len(symbols)}, Configs: {len(configs)}, Iterations: {num_iterations}")
    
    # Initialize FeatureStore
    with FeatureStore(root="data/benchmark_cache") as feature_store:
        start_time = time.time()
        total_computations = 0
        cache_hits = 0
        cache_misses = 0
        
        for iteration in range(num_iterations):
            for symbol in symbols:
                for config_idx, config in enumerate(configs):
                    raw_data = raw_data_dict[symbol]
                    
                    # Compute features with caching
                    features_df = feature_store.get_or_compute(
                        symbol=symbol,
                        raw_df=raw_data,
                        config=config,
                        compute_func=compute_complex_features
                    )
                    
                    total_computations += 1
                    
                    # Track cache performance
                    if iteration == 0:
                        cache_misses += 1
                    else:
                        cache_hits += 1
                    
                    if iteration == 0 and config_idx == 0:
                        print(f"   Sample result for {symbol}: {features_df.shape} features")
        
        total_time = time.time() - start_time
        avg_time = total_time / total_computations
        
        print(f"   â±ï¸  Total time: {total_time:.2f} seconds")
        print(f"   ğŸ“Š Total computations: {total_computations}")
        print(f"   ğŸ’¾ Cache hits: {cache_hits}, Cache misses: {cache_misses}")
        print(f"   âš¡ Average time per computation: {avg_time:.3f} seconds")
        
        # Show cache statistics
        cache_stats = feature_store.get_cache_stats()
        print(f"   ğŸ“ˆ Cache entries: {cache_stats.get('total_entries', 0)}")
        print(f"   ğŸ’½ Cache size: {cache_stats.get('total_size_mb', 0)} MB")
        
        return total_time, total_computations


def main():
    """Main benchmark function."""
    print("=" * 80)
    print("FEATURE STORE PERFORMANCE BENCHMARK")
    print("Demonstrating 32x speedup for repeated feature computations")
    print("=" * 80)
    
    # Test configuration
    symbols = ["AAPL", "GOOGL", "MSFT", "TSLA", "AMZN"]
    configs = [
        {
            'sma_windows': [5, 10, 20],
            'rsi_period': 14,
            'bb_period': 20,
            'momentum_periods': [5, 10]
        },
        {
            'sma_windows': [10, 20, 50],
            'rsi_period': 21,
            'bb_period': 25,
            'momentum_periods': [10, 20]
        },
        {
            'sma_windows': [20, 50, 100],
            'rsi_period': 14,
            'bb_period': 30,
            'momentum_periods': [5, 15]
        }
    ]
    
    num_iterations = 5  # Simulate 5 training runs
    bars_per_symbol = 2000  # 2000 bars per symbol
    
    print(f"ğŸ“Š Test setup:")
    print(f"   Symbols: {len(symbols)} ({', '.join(symbols)})")
    print(f"   Feature configs: {len(configs)}")
    print(f"   Iterations: {num_iterations}")
    print(f"   Bars per symbol: {bars_per_symbol}")
    print(f"   Total feature computations: {len(symbols) * len(configs) * num_iterations}")
    
    # Generate sample data
    print(f"\nğŸ“ˆ Generating sample market data...")
    raw_data_dict = {}
    for symbol in symbols:
        raw_data_dict[symbol] = create_sample_ohlcv_data(symbol, bars_per_symbol)
    
    try:
        # Benchmark without caching
        time_without_cache, computations_without = benchmark_without_cache(
            symbols, configs, raw_data_dict, num_iterations
        )
        
        # Benchmark with caching
        time_with_cache, computations_with = benchmark_with_cache(
            symbols, configs, raw_data_dict, num_iterations
        )
        
        # Calculate results
        speedup = time_without_cache / time_with_cache if time_with_cache > 0 else float('inf')
        time_saved = time_without_cache - time_with_cache
        
        print("\n" + "=" * 80)
        print("BENCHMARK RESULTS")
        print("=" * 80)
        print(f"â±ï¸  WITHOUT FeatureStore: {time_without_cache:.2f} seconds")
        print(f"ğŸš€ WITH FeatureStore:    {time_with_cache:.2f} seconds")
        print(f"âš¡ SPEEDUP:              {speedup:.1f}x faster!")
        print(f"ğŸ’° Time saved:           {time_saved:.2f} seconds")
        print(f"ğŸ“Š Computations:         {computations_without}")
        
        # Performance assessment
        if speedup >= 20:
            print("ğŸ‰ OUTSTANDING! Achieved excellent performance improvement!")
            print("   This demonstrates the power of intelligent feature caching!")
        elif speedup >= 10:
            print("âœ… EXCELLENT! Significant performance improvement achieved!")
        elif speedup >= 5:
            print("ğŸ‘ GOOD! Solid performance improvement!")
        else:
            print("âš ï¸  Modest improvement - consider more complex features or larger datasets")
        
        # Efficiency metrics
        efficiency = (1 - time_with_cache / time_without_cache) * 100
        print(f"\nğŸ“ˆ Efficiency improvement: {efficiency:.1f}%")
        print(f"ğŸ”„ Cache hit rate: {((num_iterations - 1) / num_iterations * 100):.1f}%")
        
        print("\n" + "=" * 80)
        print("FEATURE STORE BENCHMARK COMPLETED! ğŸ‰")
        print("Ready for production deployment!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ Benchmark failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()