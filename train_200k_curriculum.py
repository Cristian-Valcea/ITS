#!/usr/bin/env python3
"""
üöÄ 200K CURRICULUM TRAINING - STAIRWAYS V4
Full-scale training with curriculum tightening and quality gates.

Pre-Flight Checklist:
‚úÖ Checkpoint frequency: save_interval = 25 updates (‚âà ‚àö390)
‚úÖ Learning-rate decay: Linear 1e-4 ‚Üí 3e-5 across 200K
‚úÖ Entropic bonus schedule: 0.05 ‚Üí 0.02 ‚Üí 0.01
‚úÖ Curriculum drawdown caps: 20% ‚Üí 10% ‚Üí 5%
‚úÖ Gate thresholds: Sharpe ‚â• 0.3, DD < 5%, invalid = 0

Curriculum Schedule:
Phase 1 (0-50K):   DD=20%, ent_coef=0.05, lr=1e-4
Phase 2 (50-150K): DD=10%, ent_coef=0.02, lr=7e-5  
Phase 3 (150-200K): DD=5%,  ent_coef=0.01, lr=3e-5

Quality Gates:
Mid-Run (100K): ep_rew_mean ‚â• 0.75, Sharpe ‚â• 0.25, DD ‚â§ 10%
Final (200K):   ep_rew_mean ‚â• 1.00, Sharpe ‚â• 0.35, DD ‚â§ 5%
"""

import sys
import logging
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import time

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from secrets_helper import SecretsHelper
from src.gym_env.dual_ticker_data_adapter import DualTickerDataAdapter
from src.gym_env.dual_ticker_trading_env_v3_enhanced_5action import DualTickerTradingEnvV3Enhanced
from src.gym_env.refined_reward_system import RefinedRewardSystem

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    import gymnasium as gym
    from gymnasium import Wrapper
except ImportError:
    import gym
    from gym import Wrapper

class CurriculumCallback(BaseCallback):
    """Callback to handle curriculum tightening and quality gates"""
    
    def __init__(self, env, save_path, verbose=0):
        super().__init__(verbose)
        self.env = env
        self.save_path = save_path
        self.phase = 1
        self.last_checkpoint = 0
        self.checkpoint_interval = 25  # Save every 25 updates (‚âà ‚àö390)
        
        # Quality gate thresholds
        self.mid_run_gates = {
            'timestep': 100000,
            'ep_rew_mean': 0.75,
            'sharpe_min': 0.25,
            'max_dd': 0.10
        }
        
        self.final_gates = {
            'timestep': 200000,
            'ep_rew_mean': 1.00,
            'sharpe_min': 0.35,
            'max_dd': 0.05
        }
        
        # Track metrics for gates
        self.episode_rewards = []
        self.episode_lengths = []
        self.portfolio_values = []
        
    def _on_step(self) -> bool:
        """Called at each training step"""
        
        # Phase transitions based on timesteps
        if self.num_timesteps >= 150000 and self.phase < 3:
            self._transition_to_phase_3()
        elif self.num_timesteps >= 50000 and self.phase < 2:
            self._transition_to_phase_2()
        
        # Checkpoint saving
        if self.n_calls - self.last_checkpoint >= self.checkpoint_interval:
            self._save_checkpoint()
            self.last_checkpoint = self.n_calls
        
        # Quality gate checks
        if self.num_timesteps >= self.mid_run_gates['timestep'] and self.phase == 2:
            if not self._check_mid_run_gates():
                logger.error("‚ùå Mid-run quality gates failed - stopping training")
                return False
        
        if self.num_timesteps >= self.final_gates['timestep']:
            if not self._check_final_gates():
                logger.error("‚ùå Final quality gates failed - stopping training")
                return False
        
        return True
    
    def _transition_to_phase_2(self):
        """Transition to Phase 2: 50-150K"""
        logger.info("üîÑ PHASE TRANSITION: 1 ‚Üí 2 (50K-150K)")
        logger.info("   Drawdown cap: 20% ‚Üí 10%")
        logger.info("   Entropy coef: 0.05 ‚Üí 0.02")
        logger.info("   Learning rate: 1e-4 ‚Üí 7e-5")
        
        # Update model parameters
        self.model.ent_coef = 0.02
        self.model.learning_rate = 7e-5
        
        # Update environment drawdown cap (if supported)
        try:
            if hasattr(self.env.envs[0].env.env, 'max_daily_drawdown_pct'):
                self.env.envs[0].env.env.max_daily_drawdown_pct = 0.10
                logger.info("‚úÖ Environment drawdown cap updated to 10%")
        except:
            logger.warning("‚ö†Ô∏è Could not update environment drawdown cap")
        
        self.phase = 2
    
    def _transition_to_phase_3(self):
        """Transition to Phase 3: 150-200K"""
        logger.info("üîÑ PHASE TRANSITION: 2 ‚Üí 3 (150K-200K)")
        logger.info("   Drawdown cap: 10% ‚Üí 5%")
        logger.info("   Entropy coef: 0.02 ‚Üí 0.01")
        logger.info("   Learning rate: 7e-5 ‚Üí 3e-5")
        
        # Update model parameters
        self.model.ent_coef = 0.01
        self.model.learning_rate = 3e-5
        
        # Update environment drawdown cap (if supported)
        try:
            if hasattr(self.env.envs[0].env.env, 'max_daily_drawdown_pct'):
                self.env.envs[0].env.env.max_daily_drawdown_pct = 0.05
                logger.info("‚úÖ Environment drawdown cap updated to 5%")
        except:
            logger.warning("‚ö†Ô∏è Could not update environment drawdown cap")
        
        self.phase = 3
    
    def _save_checkpoint(self):
        """Save model checkpoint"""
        checkpoint_path = self.save_path / f"checkpoint_{self.num_timesteps}.zip"
        self.model.save(str(checkpoint_path))
        logger.info(f"üíæ Checkpoint saved: {checkpoint_path}")
    
    def _check_mid_run_gates(self) -> bool:
        """Check mid-run quality gates at 100K"""
        logger.info("üéØ CHECKING MID-RUN QUALITY GATES (100K)")
        
        # Get recent episode statistics from monitor
        try:
            monitor_path = self.save_path / "monitor.csv"
            if monitor_path.exists():
                import pandas as pd
                df = pd.read_csv(monitor_path, skiprows=1)
                if len(df) > 0:
                    recent_rewards = df['r'].tail(100).values
                    mean_reward = np.mean(recent_rewards)
                    sharpe = mean_reward / (np.std(recent_rewards) + 1e-8) if len(recent_rewards) > 1 else 0
                    
                    logger.info(f"   Mean Reward: {mean_reward:.3f} (target: ‚â•{self.mid_run_gates['ep_rew_mean']})")
                    logger.info(f"   Sharpe Ratio: {sharpe:.3f} (target: ‚â•{self.mid_run_gates['sharpe_min']})")
                    
                    # Check gates
                    reward_pass = mean_reward >= self.mid_run_gates['ep_rew_mean']
                    sharpe_pass = sharpe >= self.mid_run_gates['sharpe_min']
                    
                    if reward_pass and sharpe_pass:
                        logger.info("‚úÖ Mid-run quality gates PASSED")
                        return True
                    else:
                        logger.error("‚ùå Mid-run quality gates FAILED")
                        return False
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not check mid-run gates: {e}")
        
        logger.warning("‚ö†Ô∏è Insufficient data for mid-run gate check")
        return True  # Continue if no data available
    
    def _check_final_gates(self) -> bool:
        """Check final quality gates at 200K"""
        logger.info("üéØ CHECKING FINAL QUALITY GATES (200K)")
        
        # Get recent episode statistics from monitor
        try:
            monitor_path = self.save_path / "monitor.csv"
            if monitor_path.exists():
                import pandas as pd
                df = pd.read_csv(monitor_path, skiprows=1)
                if len(df) > 0:
                    recent_rewards = df['r'].tail(100).values
                    mean_reward = np.mean(recent_rewards)
                    sharpe = mean_reward / (np.std(recent_rewards) + 1e-8) if len(recent_rewards) > 1 else 0
                    
                    logger.info(f"   Mean Reward: {mean_reward:.3f} (target: ‚â•{self.final_gates['ep_rew_mean']})")
                    logger.info(f"   Sharpe Ratio: {sharpe:.3f} (target: ‚â•{self.final_gates['sharpe_min']})")
                    
                    # Check gates
                    reward_pass = mean_reward >= self.final_gates['ep_rew_mean']
                    sharpe_pass = sharpe >= self.final_gates['sharpe_min']
                    
                    if reward_pass and sharpe_pass:
                        logger.info("üéâ Final quality gates PASSED")
                        return True
                    else:
                        logger.error("‚ùå Final quality gates FAILED")
                        return False
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not check final gates: {e}")
        
        logger.warning("‚ö†Ô∏è Insufficient data for final gate check")
        return True  # Continue if no data available

class CurriculumRewardWrapper(Wrapper):
    """Wrapper with curriculum-aware reward tracking"""
    
    def __init__(self, env, refined_reward_system):
        super().__init__(env)
        self.refined_reward = refined_reward_system
        self.episode_stats = {
            'holding_bonus_sum': 0.0,
            'penalty_sum': 0.0,
            'episode_count': 0,
            'total_reward': 0.0
        }
    
    def step(self, action):
        """Step with curriculum-aware tracking"""
        obs, original_reward, done, truncated, info = self.env.step(action)
        
        # Extract state information
        portfolio_value = info.get('portfolio_value', 10000.0)
        previous_portfolio_value = getattr(self, '_prev_portfolio_value', 10000.0)
        nvda_position = info.get('nvda_position', 0.0)
        msft_position = info.get('msft_position', 0.0)
        
        # Calculate drawdown percentage
        initial_value = 10000.0
        drawdown_pct = max(0, (initial_value - portfolio_value) / initial_value)
        
        # Calculate refined reward
        reward_components = self.refined_reward.calculate_reward(
            portfolio_value=portfolio_value,
            previous_portfolio_value=previous_portfolio_value,
            nvda_position=nvda_position,
            msft_position=msft_position,
            action=action,
            drawdown_pct=drawdown_pct
        )
        
        # Track curriculum metrics
        self.episode_stats['holding_bonus_sum'] += reward_components.holding_bonus
        self.episode_stats['penalty_sum'] += reward_components.smoothed_penalty
        self.episode_stats['total_reward'] += reward_components.total_reward
        
        # Use refined reward
        refined_reward = reward_components.total_reward
        
        # Update info
        info['refined_reward_components'] = reward_components.to_dict()
        info['original_reward'] = original_reward
        
        # Store for next step
        self._prev_portfolio_value = portfolio_value
        
        # Log episode summary on termination
        if done or truncated:
            self._log_curriculum_summary()
            self._reset_episode_tracking()
        
        return obs, refined_reward, done, truncated, info
    
    def reset(self, **kwargs):
        """Reset with curriculum tracking"""
        obs, info = self.env.reset(**kwargs)
        self.refined_reward.reset_episode()
        self._prev_portfolio_value = 10000.0
        return obs, info
    
    def _log_curriculum_summary(self):
        """Log curriculum-aware episode summary"""
        self.episode_stats['episode_count'] += 1
        
        if self.episode_stats['episode_count'] % 50 == 0:  # Log every 50 episodes
            avg_holding = self.episode_stats['holding_bonus_sum'] / self.episode_stats['episode_count']
            avg_penalty = self.episode_stats['penalty_sum'] / self.episode_stats['episode_count']
            avg_reward = self.episode_stats['total_reward'] / self.episode_stats['episode_count']
            
            logger.info(f"üìä Curriculum Stats (last 50 eps): " +
                       f"avg_reward={avg_reward:.3f}, " +
                       f"avg_holding={avg_holding:.4f}, " +
                       f"avg_penalty={avg_penalty:.3f}")
    
    def _reset_episode_tracking(self):
        """Reset episode tracking periodically"""
        if self.episode_stats['episode_count'] % 100 == 0:
            # Reset stats every 100 episodes to keep recent averages
            self.episode_stats = {
                'holding_bonus_sum': 0.0,
                'penalty_sum': 0.0,
                'episode_count': 0,
                'total_reward': 0.0
            }

def main():
    """Main 200K curriculum training execution"""
    
    logger.info("üöÄ 200K CURRICULUM TRAINING - STAIRWAYS V4")
    logger.info("=" * 70)
    
    # Training configuration
    total_timesteps = 200000
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"stairways_v4_200k_{timestamp}"
    save_path = Path(f"train_runs/{run_name}")
    save_path.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"üé™ Run: {run_name}")
    logger.info(f"üìä Steps: {total_timesteps:,}")
    logger.info(f"üéØ Focus: Full curriculum training with quality gates")
    logger.info(f"‚è±Ô∏è Expected Duration: 8-10 minutes @ 500 FPS")
    
    # Log curriculum schedule
    logger.info("üìã CURRICULUM SCHEDULE:")
    logger.info("   Phase 1 (0-50K):   DD=20%, ent_coef=0.05, lr=1e-4")
    logger.info("   Phase 2 (50-150K): DD=10%, ent_coef=0.02, lr=7e-5")
    logger.info("   Phase 3 (150-200K): DD=5%,  ent_coef=0.01, lr=3e-5")
    
    logger.info("üéØ QUALITY GATES:")
    logger.info("   Mid-Run (100K): ep_rew_mean ‚â• 0.75, Sharpe ‚â• 0.25")
    logger.info("   Final (200K):   ep_rew_mean ‚â• 1.00, Sharpe ‚â• 0.35")
    
    try:
        # Load real market data
        logger.info("üìà Loading real market data...")
        db_password = SecretsHelper.get_timescaledb_password()
        db_config = {
            'host': 'localhost',
            'port': 5432,
            'database': 'trading_data', 
            'user': 'postgres',
            'password': db_password
        }
        
        adapter = DualTickerDataAdapter(db_config, live_trading_mode=False)
        market_data = adapter.load_training_data(
            start_date='2022-01-03',
            end_date='2024-12-31',
            symbols=['NVDA', 'MSFT'],
            bar_size='1min',
            data_split='train'
        )
        
        # Prepare data for environment
        nvda_features = market_data['nvda_features']
        nvda_prices = market_data['nvda_prices']
        msft_features = market_data['msft_features']
        msft_prices = market_data['msft_prices']
        trading_days = market_data['trading_days']
        
        # Combine features (26-dim observation)
        combined_features = np.concatenate([nvda_features, msft_features], axis=1)
        position_features = np.zeros((combined_features.shape[0], 2))
        combined_features = np.concatenate([combined_features, position_features], axis=1)
        
        # Create 4-column price data
        nvda_returns = np.concatenate([[0], np.diff(nvda_prices) / nvda_prices[:-1]])
        msft_returns = np.concatenate([[0], np.diff(msft_prices) / msft_prices[:-1]])
        combined_prices = np.column_stack([nvda_prices, nvda_returns, msft_prices, msft_returns])
        
        logger.info(f"üìä Data loaded: {len(trading_days)} timesteps")
        
        # Create base environment - START WITH PHASE 1 SETTINGS
        logger.info("üèóÔ∏è Creating curriculum environment (Phase 1)...")
        base_env = DualTickerTradingEnvV3Enhanced(
            processed_feature_data=combined_features,
            processed_price_data=combined_prices,
            trading_days=trading_days,
            initial_capital=10000.0,
            lookback_window=50,
            max_episode_steps=390,          # Full trading day
            max_daily_drawdown_pct=0.20,    # Phase 1: 20% (will tighten)
            transaction_cost_pct=0.001
        )
        
        # Create refined reward system with PROVEN PARAMETERS
        logger.info("üéØ Initializing refined reward system (proven parameters)...")
        refined_reward_system = RefinedRewardSystem(
            initial_capital=10000.0,
            pnl_epsilon=750.0,              # Proven from probe
            holding_alpha=0.05,             # Proven from probe
            holding_lookback_k=5,           # Proven from probe
            holding_min_ret=0.0,            # Proven from probe
            penalty_beta=0.10,              # Proven from probe
            exploration_coef=0.05,          # Phase 1 (will decay)
            exploration_decay=0.9999,       # Proven from probe
            verbose=True
        )
        
        # Wrap environment with curriculum tracking
        env = CurriculumRewardWrapper(base_env, refined_reward_system)
        env = Monitor(env, str(save_path / "monitor.csv"))
        env = DummyVecEnv([lambda: env])
        
        logger.info("‚úÖ Curriculum environment created")
        
        # Create PPO model with PHASE 1 PARAMETERS
        logger.info("ü§ñ Creating PPO model (Phase 1 parameters)...")
        model = PPO(
            policy='MlpPolicy',
            env=env,
            learning_rate=1e-4,             # Phase 1: 1e-4 (will decay)
            n_steps=512,
            batch_size=128,
            n_epochs=4,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.05,                  # Phase 1: 0.05 (will decay)
            vf_coef=0.5,
            max_grad_norm=0.5,
            policy_kwargs={'net_arch': [dict(pi=[256, 256], vf=[256, 256])]},
            tensorboard_log=f"tensorboard_logs/{run_name}",
            verbose=1
        )
        
        # Create curriculum callback
        curriculum_callback = CurriculumCallback(env, save_path, verbose=1)
        
        logger.info("‚úÖ PPO model with curriculum callback created")
        
        # Launch 200K curriculum training
        logger.info("üöÄ LAUNCHING 200K CURRICULUM TRAINING")
        logger.info("=" * 70)
        
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            callback=curriculum_callback,
            tb_log_name=run_name
        )
        
        end_time = time.time()
        duration_minutes = (end_time - start_time) / 60
        
        # Save final model
        final_model_path = save_path / "stairways_v4_final_model.zip"
        model.save(str(final_model_path))
        
        logger.info("üéâ 200K CURRICULUM TRAINING COMPLETE!")
        logger.info(f"üìÅ Final model saved: {final_model_path}")
        logger.info(f"‚è±Ô∏è Duration: {duration_minutes:.1f} minutes")
        logger.info(f"üìä TensorBoard: tensorboard --logdir tensorboard_logs/{run_name}")
        
        # Final validation
        logger.info("üß™ Running final validation...")
        
        # Quick validation episodes
        obs, _ = env.reset()
        episode_rewards = []
        episode_lengths = []
        
        for episode in range(10):
            obs, _ = env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            while not done and episode_length < 200:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, _ = env.step(action)
                episode_reward += reward[0]
                episode_length += 1
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
        
        avg_reward = np.mean(episode_rewards)
        avg_length = np.mean(episode_lengths)
        reward_std = np.std(episode_rewards)
        final_sharpe = avg_reward / (reward_std + 1e-8)
        
        logger.info("üìä FINAL VALIDATION RESULTS:")
        logger.info(f"  Average Reward: {avg_reward:.3f}")
        logger.info(f"  Average Length: {avg_length:.1f} steps")
        logger.info(f"  Reward Std: {reward_std:.3f}")
        logger.info(f"  Final Sharpe: {final_sharpe:.3f}")
        
        # Success assessment
        success_criteria = {
            'reward_excellent': avg_reward >= 1.00,
            'reward_good': avg_reward >= 0.75,
            'sharpe_excellent': final_sharpe >= 0.35,
            'sharpe_good': final_sharpe >= 0.25,
            'length_adequate': avg_length >= 50.0
        }
        
        logger.info("üéØ SUCCESS CRITERIA:")
        for criterion, passed in success_criteria.items():
            status = "‚úÖ PASS" if passed else "‚ùå FAIL"
            logger.info(f"  {criterion}: {status}")
        
        # Overall assessment
        if success_criteria['reward_excellent'] and success_criteria['sharpe_excellent']:
            logger.info("üéâ STAIRWAYS V4: EXCELLENT SUCCESS")
            return "excellent"
        elif success_criteria['reward_good'] and success_criteria['sharpe_good']:
            logger.info("‚úÖ STAIRWAYS V4: SUCCESS")
            return "success"
        else:
            logger.warning("‚ö†Ô∏è STAIRWAYS V4: PARTIAL SUCCESS")
            return "partial"
        
    except Exception as e:
        logger.error(f"‚ùå 200K curriculum training failed: {e}")
        import traceback
        traceback.print_exc()
        return "failed"

if __name__ == "__main__":
    result = main()
    
    if result == "excellent":
        print("üéâ 200K STAIRWAYS V4 CURRICULUM: EXCELLENT SUCCESS")
        print("üöÄ Ready for live-bar fine-tuning and paper trading")
        sys.exit(0)
    elif result == "success":
        print("‚úÖ 200K STAIRWAYS V4 CURRICULUM: SUCCESS")
        print("üöÄ Ready for paper trading validation")
        sys.exit(0)
    elif result == "partial":
        print("‚ö†Ô∏è 200K STAIRWAYS V4 CURRICULUM: PARTIAL SUCCESS")
        print("üîß Consider additional fine-tuning")
        sys.exit(1)
    else:
        print("‚ùå 200K STAIRWAYS V4 CURRICULUM: FAILED")
        sys.exit(1)