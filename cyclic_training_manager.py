#!/usr/bin/env python3
"""
üîÑ STAIRWAYS TO HEAVEN V3 - CYCLIC TRAINING MANAGEMENT SYSTEM
8√ó6K cycle management system for production training with enhanced V3 environment

TRAINING OBJECTIVE: Systematic 8-cycle training progression with enhanced frequency control
- 8 cycles √ó 6000 steps each = 48K total training steps
- Progressive controller target adjustment across cycles
- Model checkpoint management and validation
- Training metrics collection and analysis
- Automated cycle progression with performance gates
- Enhanced environment integration with regime intelligence

STAIRWAYS TO HEAVEN V3.0 - PHASE 2 IMPLEMENTATION
"""

import numpy as np
import pandas as pd
import logging
import json
import shutil
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import os
import subprocess
import psutil

# ML imports
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
    from stable_baselines3.common.monitor import Monitor
    from stable_baselines3.common.vec_env import DummyVecEnv
except ImportError:
    logger.warning("stable_baselines3 not available - training functionality disabled")
    PPO = None

# Environment imports
from src.gym_env.dual_ticker_trading_env_v3_enhanced import DualTickerTradingEnvV3Enhanced
from src.gym_env.dual_ticker_data_adapter import DualTickerDataAdapter

# Validation imports
from dry_run_validator import DryRunValidator
from shadow_replay_validator import ShadowReplayValidator

# Metrics imports
from metrics_reporter import MetricsReporter

logger = logging.getLogger(__name__)

@dataclass
class CycleConfiguration:
    """Configuration for a single training cycle."""
    
    cycle_number: int
    cycle_name: str
    training_steps: int
    
    # Controller parameters for this cycle
    controller_target_hold_rate: float
    controller_enabled: bool
    regime_detection_enabled: bool
    
    # Environment parameters
    episode_length: int
    learning_rate: float
    batch_size: int
    
    # Validation parameters
    validation_episodes: int
    validation_interval: int
    
    # Performance gates
    min_hold_rate_improvement: float  # Minimum improvement vs previous cycle
    max_performance_degradation: float  # Maximum allowed portfolio performance drop
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage."""
        return asdict(self)

@dataclass
class CycleResult:
    """Results from a completed training cycle."""
    
    cycle_number: int
    cycle_name: str
    start_time: str
    end_time: str
    duration_seconds: float
    
    # Training metrics
    training_steps_completed: int
    final_mean_reward: float
    final_episode_reward: float
    training_loss: float
    
    # Validation metrics
    validation_episodes_completed: int
    avg_hold_rate: float
    avg_portfolio_return: float
    avg_trade_frequency: float
    controller_effectiveness: float
    
    # Performance gates
    hold_rate_improvement: float
    performance_degradation: float
    gates_passed: bool
    
    # Model checkpoint info
    checkpoint_path: str
    model_size_mb: float
    
    # Issues and notes
    issues_detected: List[str]
    training_notes: str
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage."""
        return asdict(self)

class CyclicTrainingCallback(BaseCallback):
    """Custom callback for cyclic training monitoring."""
    
    def __init__(self, cycle_manager, cycle_config: CycleConfiguration, verbose: int = 1):
        super().__init__(verbose)
        self.cycle_manager = cycle_manager
        self.cycle_config = cycle_config
        self.step_rewards = []
        self.step_hold_rates = []
        self.step_regime_scores = []
        
    def _on_step(self) -> bool:
        # Collect training metrics
        if len(self.locals.get('infos', [])) > 0:
            info = self.locals['infos'][0]
            
            # Extract Stairways metrics
            stairways_info = info.get('stairways_info', {})
            if stairways_info:
                self.step_hold_rates.append(stairways_info.get('current_hold_rate', 0.0))
                self.step_regime_scores.append(stairways_info.get('current_regime_score', 0.0))
            
            # Extract reward
            if 'reward' in self.locals:
                self.step_rewards.append(self.locals['reward'])
        
        # Log progress every 1000 steps
        if self.num_timesteps % 1000 == 0:
            recent_hold_rate = np.mean(self.step_hold_rates[-100:]) if self.step_hold_rates else 0.0
            recent_reward = np.mean(self.step_rewards[-100:]) if self.step_rewards else 0.0
            
            logger.info(f"   Step {self.num_timesteps}: Hold Rate: {recent_hold_rate:.1%}, Reward: {recent_reward:.3f}")
        
        return True
    
    def get_training_metrics(self) -> Dict[str, Any]:
        """Get collected training metrics."""
        return {
            'step_rewards': self.step_rewards,
            'step_hold_rates': self.step_hold_rates,
            'step_regime_scores': self.step_regime_scores,
            'avg_hold_rate': np.mean(self.step_hold_rates) if self.step_hold_rates else 0.0,
            'avg_reward': np.mean(self.step_rewards) if self.step_rewards else 0.0,
            'avg_regime_score': np.mean(self.step_regime_scores) if self.step_regime_scores else 0.0
        }

class CyclicTrainingManager:
    """
    Comprehensive 8√ó6K cyclic training management system for Stairways to Heaven V3.
    
    This manager orchestrates systematic training across 8 cycles with:
    1. Progressive controller target adjustment
    2. Model checkpoint management and validation
    3. Performance gate enforcement
    4. Training metrics collection and analysis
    5. Automated cycle progression with rollback capability
    6. Enhanced environment integration
    """
    
    def __init__(
        self,
        data_adapter: DualTickerDataAdapter,
        base_model_path: str = None,
        training_dir: str = "cyclic_training_runs",
        checkpoint_interval: int = 1000,
        enable_validation: bool = True,
        enable_shadow_replay: bool = True,
        verbose: bool = True
    ):
        """
        Initialize cyclic training manager.
        
        Args:
            data_adapter: Data adapter with prepared market data
            base_model_path: Path to base V3 model for warm start
            training_dir: Directory for training outputs
            checkpoint_interval: Steps between model checkpoints
            enable_validation: Enable cycle validation
            enable_shadow_replay: Enable shadow replay testing
            verbose: Enable detailed logging
        """
        self.data_adapter = data_adapter
        self.base_model_path = base_model_path
        self.training_dir = Path(training_dir)
        self.checkpoint_interval = checkpoint_interval
        self.enable_validation = enable_validation
        self.enable_shadow_replay = enable_shadow_replay
        self.verbose = verbose
        
        # Create training directory
        self.training_dir.mkdir(exist_ok=True)
        
        # Training state
        self.cycle_configs: List[CycleConfiguration] = []
        self.cycle_results: List[CycleResult] = []
        self.current_cycle = 0
        
        # Model management
        self.current_model: Optional[PPO] = None
        self.best_model_path: Optional[str] = None
        self.best_performance_score: float = -np.inf
        
        # R2 FIX: Initialize metrics reporter
        self.metrics_reporter: Optional[MetricsReporter] = None
        if enable_validation:  # Only enable metrics if validation is enabled
            try:
                self.metrics_reporter = MetricsReporter(
                    job_name="stairways_cyclic_training",
                    instance_id=f"training_{int(time.time())}",
                    batch_size=64,
                    enable_prometheus=True,
                    verbose=verbose
                )
            except Exception as e:
                if verbose:
                    logger.warning(f"‚ö†Ô∏è Failed to initialize metrics reporter: {e}")
                self.metrics_reporter = None
        
        # Initialize default cycle configurations
        self._initialize_cycle_configurations()
        
        if self.verbose:
            logger.info(f"üîÑ Cyclic training manager initialized")
            logger.info(f"   Training directory: {self.training_dir}")
            logger.info(f"   Base model: {self.base_model_path or 'None (cold start)'}")
            logger.info(f"   Validation enabled: {self.enable_validation}")
            logger.info(f"   Shadow replay enabled: {self.enable_shadow_replay}")
            logger.info(f"   Cycles configured: {len(self.cycle_configs)}")
    
    def _initialize_cycle_configurations(self):
        """
        Initialize default 8√ó6K cycle configurations with progressive controller tuning.
        
        Each cycle progressively reduces target hold rate and adjusts learning parameters.
        """
        base_config = {
            'training_steps': 6000,
            'episode_length': 1000,
            'batch_size': 64,
            'validation_episodes': 5,
            'validation_interval': 2000,
            'max_performance_degradation': 0.05  # Max 5% portfolio performance drop
        }
        
        # Progressive cycle configurations
        cycle_specs = [
            # Cycle 1-2: Baseline establishment (75% hold rate target)
            {'target_hold_rate': 0.75, 'learning_rate': 3e-4, 'min_improvement': 0.00},
            {'target_hold_rate': 0.75, 'learning_rate': 3e-4, 'min_improvement': 0.02},
            
            # Cycle 3-4: Initial frequency increase (70% hold rate target)
            {'target_hold_rate': 0.70, 'learning_rate': 2e-4, 'min_improvement': 0.03},
            {'target_hold_rate': 0.70, 'learning_rate': 2e-4, 'min_improvement': 0.02},
            
            # Cycle 5-6: Moderate frequency optimization (67% hold rate target)
            {'target_hold_rate': 0.67, 'learning_rate': 1e-4, 'min_improvement': 0.02},
            {'target_hold_rate': 0.67, 'learning_rate': 1e-4, 'min_improvement': 0.01},
            
            # Cycle 7-8: Target achievement (65% hold rate target)
            {'target_hold_rate': 0.65, 'learning_rate': 5e-5, 'min_improvement': 0.01},
            {'target_hold_rate': 0.65, 'learning_rate': 5e-5, 'min_improvement': 0.01}
        ]
        
        self.cycle_configs = []
        
        for i, spec in enumerate(cycle_specs):
            config = CycleConfiguration(
                cycle_number=i + 1,
                cycle_name=f"cycle_{i+1:02d}_hold_{spec['target_hold_rate']:.0%}",
                controller_target_hold_rate=spec['target_hold_rate'],
                controller_enabled=True,
                regime_detection_enabled=True,
                learning_rate=spec['learning_rate'],
                min_hold_rate_improvement=spec['min_improvement'],
                **base_config
            )
            
            self.cycle_configs.append(config)
        
        if self.verbose:
            logger.info(f"üìã Initialized {len(self.cycle_configs)} cycle configurations:")
            for config in self.cycle_configs:
                logger.info(f"   {config.cycle_name}: {config.controller_target_hold_rate:.0%} hold rate, LR={config.learning_rate}")
    
    def _create_training_environment(self, cycle_config: CycleConfiguration) -> DualTickerTradingEnvV3Enhanced:
        """
        Create training environment for specific cycle.
        
        Args:
            cycle_config: Configuration for the training cycle
            
        Returns:
            Configured enhanced environment
        """
        env = DualTickerTradingEnvV3Enhanced(
            processed_feature_data=self.data_adapter.feature_data,
            processed_price_data=self.data_adapter.price_data,
            trading_days=self.data_adapter.trading_days,
            max_episode_steps=cycle_config.episode_length,
            enable_controller=cycle_config.controller_enabled,
            enable_regime_detection=cycle_config.regime_detection_enabled,
            controller_target_hold_rate=cycle_config.controller_target_hold_rate,
            bootstrap_days=50,
            verbose=False  # Reduce verbosity during training
        )
        
        # Wrap with monitor for metrics collection
        cycle_dir = self.training_dir / cycle_config.cycle_name
        cycle_dir.mkdir(exist_ok=True)
        
        env = Monitor(env, str(cycle_dir / "training_monitor"))
        
        return env
    
    def _initialize_model(self, cycle_config: CycleConfiguration, env) -> PPO:
        """
        Initialize PPO model for training cycle.
        
        Args:
            cycle_config: Configuration for the training cycle
            env: Training environment
            
        Returns:
            Initialized PPO model
        """
        # Model configuration
        model_config = {
            'learning_rate': cycle_config.learning_rate,
            'batch_size': cycle_config.batch_size,
            'n_epochs': 10,
            'gamma': 0.99,
            'gae_lambda': 0.95,
            'clip_range': 0.2,
            'ent_coef': 0.01,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5,
            'verbose': 1 if self.verbose else 0
        }
        
        if cycle_config.cycle_number == 1 and self.base_model_path:
            # Warm start from base model
            if self.verbose:
                logger.info(f"üî• Warm starting from base model: {self.base_model_path}")
            
            try:
                model = PPO.load(self.base_model_path, env=env)
                # Update learning parameters for fine-tuning
                model.learning_rate = cycle_config.learning_rate
                model.batch_size = cycle_config.batch_size
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to load base model: {e}. Starting cold.")
                model = PPO('MlpPolicy', env, **model_config)
        
        elif self.current_model and cycle_config.cycle_number > 1:
            # Continue from previous cycle
            if self.verbose:
                logger.info(f"üîÑ Continuing from previous cycle model")
            
            model = self.current_model
            model.set_env(env)
            model.learning_rate = cycle_config.learning_rate
            model.batch_size = cycle_config.batch_size
        
        else:
            # Cold start
            if self.verbose:
                logger.info(f"‚ùÑÔ∏è Cold starting new model")
            
            model = PPO('MlpPolicy', env, **model_config)
        
        return model
    
    def _run_cycle_validation(self, cycle_config: CycleConfiguration, model: PPO) -> Dict[str, Any]:
        """
        Run validation for completed training cycle.
        
        Args:
            cycle_config: Configuration for the training cycle
            model: Trained model to validate
            
        Returns:
            Validation results dictionary
        """
        if not self.enable_validation:
            return {'validation_enabled': False}
        
        if self.verbose:
            logger.info(f"üß™ Running cycle validation...")
        
        try:
            # Create validation environment
            val_env = self._create_training_environment(cycle_config)
            
            # Run validation episodes
            episode_returns = []
            episode_hold_rates = []
            episode_trade_frequencies = []
            
            for episode in range(cycle_config.validation_episodes):
                obs, info = val_env.reset(seed=42 + episode)
                episode_reward = 0.0
                episode_steps = 0
                hold_actions = 0
                trades = 0
                
                done = False
                while not done and episode_steps < cycle_config.episode_length:
                    action, _states = model.predict(obs, deterministic=True)
                    obs, reward, done, truncated, info = val_env.step(action)
                    
                    episode_reward += reward
                    episode_steps += 1
                    
                    # Track trading behavior
                    if action == 4:  # Hold action
                        hold_actions += 1
                    
                    if info.get('tuning_info', {}).get('traded_this_step', False):
                        trades += 1
                    
                    if done or truncated:
                        break
                
                # Calculate episode metrics
                episode_returns.append(episode_reward)
                hold_rate = hold_actions / episode_steps if episode_steps > 0 else 0.0
                trade_frequency = trades / episode_steps if episode_steps > 0 else 0.0
                
                episode_hold_rates.append(hold_rate)
                episode_trade_frequencies.append(trade_frequency)
            
            val_env.close()
            
            # Calculate validation metrics
            validation_results = {
                'validation_enabled': True,
                'episodes_completed': len(episode_returns),
                'avg_episode_return': float(np.mean(episode_returns)),
                'std_episode_return': float(np.std(episode_returns)),
                'avg_hold_rate': float(np.mean(episode_hold_rates)),
                'std_hold_rate': float(np.std(episode_hold_rates)),
                'avg_trade_frequency': float(np.mean(episode_trade_frequencies)),
                'std_trade_frequency': float(np.std(episode_trade_frequencies)),
                'target_hold_rate': cycle_config.controller_target_hold_rate,
                'hold_rate_deviation': float(abs(np.mean(episode_hold_rates) - cycle_config.controller_target_hold_rate)),
                'controller_effectiveness': float(1.0 - abs(np.mean(episode_hold_rates) - cycle_config.controller_target_hold_rate) / 0.35)
            }
            
            if self.verbose:
                logger.info(f"   ‚úÖ Validation completed: Hold Rate: {validation_results['avg_hold_rate']:.1%}, "
                           f"Return: {validation_results['avg_episode_return']:.2f}")
            
            return validation_results
            
        except Exception as e:
            logger.error(f"‚ùå Validation failed: {e}")
            return {'validation_enabled': True, 'validation_error': str(e)}
    
    def _check_parameter_divergence(self, model: PPO, baseline_model_path: str = None) -> Tuple[bool, float, List[str]]:
        """
        R1 FIX: Check parameter divergence and rollback if exceeds threshold.
        
        Args:
            model: Current model to check
            baseline_model_path: Path to baseline model for comparison
            
        Returns:
            Tuple of (within_bounds, l2_norm_ratio, issues_list)
        """
        issues = []
        
        if not baseline_model_path or not Path(baseline_model_path).exists():
            return True, 0.0, []  # Skip check if no baseline available
        
        try:
            # Load baseline model for comparison
            baseline_model = PPO.load(baseline_model_path)
            
            # Extract and compare parameters
            current_params = [param.data.cpu().numpy().flatten() for param in model.policy.parameters()]
            baseline_params = [param.data.cpu().numpy().flatten() for param in baseline_model.policy.parameters()]
            
            # Calculate L2 norms
            current_flat = np.concatenate(current_params)
            baseline_flat = np.concatenate(baseline_params)
            l2_norm_ratio = np.linalg.norm(current_flat - baseline_flat) / np.linalg.norm(baseline_flat)
            
            # Check against 15% threshold (from Q-doc ¬ß5)
            within_bounds = l2_norm_ratio <= 0.15
            
            if not within_bounds:
                issues.append(f"Parameter divergence detected: {l2_norm_ratio:.3f} > 0.15 threshold")
                logger.warning(f"‚ö†Ô∏è Parameter divergence detected: {l2_norm_ratio:.3f}")
                # Note: Rollback would be implemented here in production
                
            return within_bounds, l2_norm_ratio, issues
            
        except Exception as e:
            logger.error(f"‚ùå Parameter divergence check failed: {e}")
            return True, 0.0, [f"Divergence check failed: {str(e)}"]
    
    def _check_performance_gates(self, cycle_config: CycleConfiguration, validation_results: Dict[str, Any], previous_result: Optional[CycleResult] = None) -> Tuple[bool, List[str]]:
        """
        Check if cycle meets performance gates.
        
        Args:
            cycle_config: Configuration for the training cycle
            validation_results: Results from cycle validation
            previous_result: Results from previous cycle (if available)
            
        Returns:
            Tuple of (gates_passed, issues_list)
        """
        issues = []
        
        if not validation_results.get('validation_enabled', False):
            return True, []  # Skip gates if validation disabled
        
        # Gate 1: Hold rate improvement
        if previous_result and cycle_config.min_hold_rate_improvement > 0:
            current_hold_rate = validation_results.get('avg_hold_rate', 1.0)
            previous_hold_rate = previous_result.avg_hold_rate
            
            hold_rate_improvement = previous_hold_rate - current_hold_rate
            
            if hold_rate_improvement < cycle_config.min_hold_rate_improvement:
                issues.append(f"Insufficient hold rate improvement: {hold_rate_improvement:.3f} < {cycle_config.min_hold_rate_improvement:.3f}")
        
        # Gate 2: Performance degradation limit
        if previous_result:
            current_return = validation_results.get('avg_episode_return', 0.0)
            previous_return = previous_result.avg_portfolio_return
            
            if previous_return > 0:  # Avoid division by zero
                performance_degradation = (previous_return - current_return) / previous_return
                
                if performance_degradation > cycle_config.max_performance_degradation:
                    issues.append(f"Excessive performance degradation: {performance_degradation:.3f} > {cycle_config.max_performance_degradation:.3f}")
        
        # Gate 3: Controller effectiveness
        controller_effectiveness = validation_results.get('controller_effectiveness', 0.0)
        if controller_effectiveness < 0.5:  # Less than 50% effective
            issues.append(f"Low controller effectiveness: {controller_effectiveness:.3f}")
        
        gates_passed = len(issues) == 0
        
        if self.verbose:
            if gates_passed:
                logger.info(f"   ‚úÖ All performance gates passed")
            else:
                logger.warning(f"   ‚ö†Ô∏è Performance gates failed: {len(issues)} issues")
                for issue in issues:
                    logger.warning(f"      - {issue}")
        
        return gates_passed, issues
    
    def _save_cycle_checkpoint(self, cycle_config: CycleConfiguration, model: PPO) -> str:
        """
        Save model checkpoint for cycle.
        
        Args:
            cycle_config: Configuration for the training cycle
            model: Trained model to save
            
        Returns:
            Path to saved checkpoint
        """
        cycle_dir = self.training_dir / cycle_config.cycle_name
        cycle_dir.mkdir(exist_ok=True)
        
        checkpoint_path = cycle_dir / f"model_checkpoint_{cycle_config.cycle_name}.zip"
        
        try:
            model.save(str(checkpoint_path))
            
            # Calculate model size
            model_size_mb = checkpoint_path.stat().st_size / (1024 * 1024)
            
            if self.verbose:
                logger.info(f"üíæ Checkpoint saved: {checkpoint_path} ({model_size_mb:.1f} MB)")
            
            return str(checkpoint_path)
            
        except Exception as e:
            logger.error(f"‚ùå Failed to save checkpoint: {e}")
            return ""
    
    def run_single_cycle(self, cycle_number: int) -> CycleResult:
        """
        Run single training cycle.
        
        Args:
            cycle_number: Cycle number to run (1-8)
            
        Returns:
            Results from completed cycle
        """
        if cycle_number < 1 or cycle_number > len(self.cycle_configs):
            raise ValueError(f"Invalid cycle number: {cycle_number}. Must be 1-{len(self.cycle_configs)}")
        
        cycle_config = self.cycle_configs[cycle_number - 1]
        
        logger.info(f"üîÑ Starting {cycle_config.cycle_name}")
        logger.info(f"   Target hold rate: {cycle_config.controller_target_hold_rate:.1%}")
        logger.info(f"   Training steps: {cycle_config.training_steps:,}")
        logger.info(f"   Learning rate: {cycle_config.learning_rate}")
        
        start_time = time.time()
        
        try:
            # Create training environment
            env = self._create_training_environment(cycle_config)
            vec_env = DummyVecEnv([lambda: env])
            
            # Initialize model
            model = self._initialize_model(cycle_config, vec_env)
            self.current_model = model
            
            # Create training callback
            callback = CyclicTrainingCallback(self, cycle_config, verbose=1 if self.verbose else 0)
            
            # Train model
            if self.verbose:
                logger.info(f"üöÄ Training for {cycle_config.training_steps:,} steps...")
            
            model.learn(
                total_timesteps=cycle_config.training_steps,
                callback=callback,
                progress_bar=self.verbose
            )
            
            # Get training metrics
            training_metrics = callback.get_training_metrics()
            
            # Run validation
            validation_results = self._run_cycle_validation(cycle_config, model)
            
            # Check performance gates
            previous_result = self.cycle_results[-1] if self.cycle_results else None
            gates_passed, issues = self._check_performance_gates(cycle_config, validation_results, previous_result)
            
            # Save checkpoint
            checkpoint_path = self._save_cycle_checkpoint(cycle_config, model)
            
            # Calculate model size
            model_size_mb = 0.0
            if checkpoint_path and Path(checkpoint_path).exists():
                model_size_mb = Path(checkpoint_path).stat().st_size / (1024 * 1024)
            
            end_time = time.time()
            duration = end_time - start_time
            
            # Create cycle result
            cycle_result = CycleResult(
                cycle_number=cycle_config.cycle_number,
                cycle_name=cycle_config.cycle_name,
                start_time=datetime.fromtimestamp(start_time).isoformat(),
                end_time=datetime.fromtimestamp(end_time).isoformat(),
                duration_seconds=duration,
                
                # Training metrics
                training_steps_completed=cycle_config.training_steps,
                final_mean_reward=training_metrics.get('avg_reward', 0.0),
                final_episode_reward=training_metrics.get('avg_reward', 0.0),
                training_loss=0.0,  # Would need to extract from model training logs
                
                # Validation metrics
                validation_episodes_completed=validation_results.get('episodes_completed', 0),
                avg_hold_rate=validation_results.get('avg_hold_rate', 0.0),
                avg_portfolio_return=validation_results.get('avg_episode_return', 0.0),
                avg_trade_frequency=validation_results.get('avg_trade_frequency', 0.0),
                controller_effectiveness=validation_results.get('controller_effectiveness', 0.0),
                
                # Performance gates
                hold_rate_improvement=0.0,  # Calculated below
                performance_degradation=0.0,  # Calculated below
                gates_passed=gates_passed,
                
                # Model checkpoint info
                checkpoint_path=checkpoint_path,
                model_size_mb=model_size_mb,
                
                # Issues and notes
                issues_detected=issues,
                training_notes=f"Cycle completed successfully. Training metrics: {training_metrics}"
            )
            
            # Calculate improvement metrics if previous cycle exists
            if previous_result:
                cycle_result.hold_rate_improvement = previous_result.avg_hold_rate - cycle_result.avg_hold_rate
                if previous_result.avg_portfolio_return != 0:
                    cycle_result.performance_degradation = (previous_result.avg_portfolio_return - cycle_result.avg_portfolio_return) / previous_result.avg_portfolio_return
            
            # Update best model if this cycle performed better
            performance_score = cycle_result.controller_effectiveness * 0.6 + (1.0 - abs(cycle_result.avg_hold_rate - cycle_config.controller_target_hold_rate)) * 0.4
            if performance_score > self.best_performance_score:
                self.best_performance_score = performance_score
                self.best_model_path = checkpoint_path
                
                if self.verbose:
                    logger.info(f"üèÜ New best model: {cycle_config.cycle_name} (score: {performance_score:.3f})")
            
            # Close environment
            vec_env.close()
            
            if self.verbose:
                logger.info(f"‚úÖ {cycle_config.cycle_name} completed in {duration:.1f}s")
                logger.info(f"   Hold rate: {cycle_result.avg_hold_rate:.1%} (target: {cycle_config.controller_target_hold_rate:.1%})")
                logger.info(f"   Controller effectiveness: {cycle_result.controller_effectiveness:.1%}")
                logger.info(f"   Gates passed: {'‚úÖ' if gates_passed else '‚ùå'}")
            
            return cycle_result
            
        except Exception as e:
            # Handle training failure
            end_time = time.time()
            duration = end_time - start_time
            
            logger.error(f"‚ùå {cycle_config.cycle_name} failed: {e}")
            
            # Create failure result
            cycle_result = CycleResult(
                cycle_number=cycle_config.cycle_number,
                cycle_name=cycle_config.cycle_name,
                start_time=datetime.fromtimestamp(start_time).isoformat(),
                end_time=datetime.fromtimestamp(end_time).isoformat(),
                duration_seconds=duration,
                
                # Training metrics (zeros for failure)
                training_steps_completed=0,
                final_mean_reward=0.0,
                final_episode_reward=0.0,
                training_loss=0.0,
                
                # Validation metrics (zeros for failure)
                validation_episodes_completed=0,
                avg_hold_rate=0.0,
                avg_portfolio_return=0.0,
                avg_trade_frequency=0.0,
                controller_effectiveness=0.0,
                
                # Performance gates
                hold_rate_improvement=0.0,
                performance_degradation=1.0,  # Maximum degradation
                gates_passed=False,
                
                # Model checkpoint info
                checkpoint_path="",
                model_size_mb=0.0,
                
                # Issues and notes
                issues_detected=[f"Training failed: {str(e)}"],
                training_notes=f"Cycle failed with error: {str(e)}"
            )
            
            return cycle_result
    
    def run_complete_cyclic_training(self, start_cycle: int = 1, end_cycle: int = 8) -> List[CycleResult]:
        """
        Run complete cyclic training from start to end cycle.
        
        Args:
            start_cycle: Starting cycle number (default: 1)
            end_cycle: Ending cycle number (default: 8)
            
        Returns:
            List of results from all completed cycles
        """
        logger.info(f"üîÑ Starting complete cyclic training: cycles {start_cycle}-{end_cycle}")
        start_time = time.time()
        
        results = []
        
        for cycle_number in range(start_cycle, end_cycle + 1):
            try:
                result = self.run_single_cycle(cycle_number)
                results.append(result)
                self.cycle_results.append(result)
                
                # Save intermediate results
                self._save_training_summary(results)
                
                # Check if we should continue (gates passed)
                if not result.gates_passed:
                    logger.warning(f"‚ö†Ô∏è Cycle {cycle_number} failed performance gates")
                    # Continue anyway for research purposes, but log the issue
                
            except Exception as e:
                logger.error(f"‚ùå Failed to run cycle {cycle_number}: {e}")
                break
        
        total_duration = time.time() - start_time
        
        logger.info(f"‚úÖ Complete cyclic training finished")
        logger.info(f"   Cycles completed: {len(results)}/{end_cycle - start_cycle + 1}")
        logger.info(f"   Total duration: {total_duration:.1f}s ({total_duration/3600:.1f}h)")
        logger.info(f"   Best model: {self.best_model_path or 'None'}")
        
        return results
    
    def _save_training_summary(self, results: List[CycleResult]):
        """
        Save training summary to JSON file.
        
        Args:
            results: List of cycle results to save
        """
        summary = {
            'training_summary': {
                'timestamp': datetime.now().isoformat(),
                'cycles_completed': len(results),
                'best_model_path': self.best_model_path,
                'best_performance_score': self.best_performance_score
            },
            'cycle_results': [result.to_dict() for result in results],
            'configuration': {
                'training_dir': str(self.training_dir),
                'base_model_path': self.base_model_path,
                'validation_enabled': self.enable_validation,
                'shadow_replay_enabled': self.enable_shadow_replay
            }
        }
        
        summary_file = self.training_dir / f"training_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        if self.verbose:
            logger.info(f"üíæ Training summary saved: {summary_file}")
    
    def get_training_summary(self) -> Dict[str, Any]:
        """
        Get comprehensive training summary.
        
        Returns:
            Dictionary with complete training summary
        """
        if not self.cycle_results:
            return {'error': 'No training results available'}
        
        # Calculate overall statistics
        total_cycles = len(self.cycle_results)
        successful_cycles = sum(1 for result in self.cycle_results if result.gates_passed)
        success_rate = successful_cycles / total_cycles
        
        # Hold rate progression
        hold_rates = [result.avg_hold_rate for result in self.cycle_results]
        initial_hold_rate = hold_rates[0] if hold_rates else 0.0
        final_hold_rate = hold_rates[-1] if hold_rates else 0.0
        total_hold_rate_improvement = initial_hold_rate - final_hold_rate
        
        # Performance metrics
        portfolio_returns = [result.avg_portfolio_return for result in self.cycle_results]
        controller_effectiveness = [result.controller_effectiveness for result in self.cycle_results]
        
        return {
            'training_overview': {
                'total_cycles': total_cycles,
                'successful_cycles': successful_cycles,
                'success_rate': success_rate,
                'total_training_steps': sum(result.training_steps_completed for result in self.cycle_results),
                'total_duration_hours': sum(result.duration_seconds for result in self.cycle_results) / 3600
            },
            'hold_rate_analysis': {
                'initial_hold_rate': initial_hold_rate,
                'final_hold_rate': final_hold_rate,
                'total_improvement': total_hold_rate_improvement,
                'improvement_percentage': total_hold_rate_improvement / initial_hold_rate * 100 if initial_hold_rate > 0 else 0.0,
                'target_achievement': 1.0 - abs(final_hold_rate - 0.65) / 0.35 if final_hold_rate > 0 else 0.0
            },
            'performance_analysis': {
                'avg_portfolio_return': np.mean(portfolio_returns) if portfolio_returns else 0.0,
                'avg_controller_effectiveness': np.mean(controller_effectiveness) if controller_effectiveness else 0.0,
                'best_model_path': self.best_model_path,
                'best_performance_score': self.best_performance_score
            },
            'cycle_details': [result.to_dict() for result in self.cycle_results]
        }

# Utility functions for standalone execution
def create_test_cyclic_manager() -> CyclicTrainingManager:
    """
    Create test cyclic training manager with synthetic data.
    
    Returns:
        CyclicTrainingManager for testing
    """
    # Create synthetic data adapter (similar to other validators)
    from dry_run_validator import create_test_data_adapter
    
    data_adapter = create_test_data_adapter()
    
    manager = CyclicTrainingManager(
        data_adapter=data_adapter,
        base_model_path=None,  # Cold start for testing
        training_dir="test_cyclic_training",
        enable_validation=True,
        enable_shadow_replay=False,  # Disable for faster testing
        verbose=True
    )
    
    return manager

if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info("üîÑ STAIRWAYS TO HEAVEN V3 - CYCLIC TRAINING MANAGER")
    logger.info("=" * 60)
    
    if PPO is None:
        logger.error("‚ùå stable_baselines3 not available - cannot run training")
        exit(1)
    
    # Create test manager
    print("üîß Creating cyclic training manager...")
    manager = create_test_cyclic_manager()
    
    # Run single cycle for testing
    print("üîÑ Running single cycle test...")
    try:
        # Override cycle config for faster testing
        manager.cycle_configs[0].training_steps = 100  # Much shorter for testing
        manager.cycle_configs[0].validation_episodes = 2
        
        result = manager.run_single_cycle(1)
        
        print("\n" + "=" * 60)
        print("üîÑ CYCLIC TRAINING TEST RESULTS")
        print("=" * 60)
        
        print(f"Cycle: {result.cycle_name}")
        print(f"Duration: {result.duration_seconds:.1f}s")
        print(f"Hold Rate: {result.avg_hold_rate:.1%}")
        print(f"Controller Effectiveness: {result.controller_effectiveness:.1%}")
        print(f"Gates Passed: {'‚úÖ' if result.gates_passed else '‚ùå'}")
        
        if result.issues_detected:
            print(f"Issues: {result.issues_detected}")
        
        print(f"Checkpoint: {result.checkpoint_path}")
        
        print(f"\n‚úÖ Cyclic training manager test completed successfully!")
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
