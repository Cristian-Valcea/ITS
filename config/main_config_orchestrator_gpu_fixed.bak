data_augmentation:
  noise_injection: true
  noise_level: 0.02
  random_scaling: true
  scale_max: 1.02
  scale_min: 0.98
  window_size: 60
  window_slicing: false
ibkr_conn:
  host: 127.0.0.1
  port: 7497
  clientId: 103
  timeout: 30
  use_cache: true
  cache_duration_minutes: 5
  simulation_mode: true
data_preprocessing:
  impute_missing: true
  normalize: false
feature_store:
  force_duckdb: true  # Skip PostgreSQL for training jobs
environment:
  initial_capital: 50000.0
  log_trades_in_env: true
  position_sizing_pct_capital: 0.25  # Legacy percentage-based sizing
  equity_scaling_factor: 0.02       # k factor for equity-scaled sizing: shares = k * portfolio_value / price
  reward_scaling: 0.01    # Scale rewards for optimal gradient signals (100x increase from 0.0001)
  trade_cooldown_steps: 12  # STRONG THROTTLE: Require 12 bars (12 minutes) between trades
  transaction_cost_pct: 0.001
  action_change_penalty_factor: 0.001  # L2 penalty for action changes to discourage ping-ponging
  # Reward shaping parameters
  turnover_bonus_threshold: 0.8        # Bonus when turnover < 80% of cap
  turnover_bonus_factor: 0.001         # Bonus amount per step when under threshold
  # Enhanced Kyle Lambda fill simulation for realistic market impact
  kyle_lambda_fills:
    enable_kyle_lambda_fills: true
    fill_simulator_config:
      lookback_period: 50
      min_periods: 10
      impact_decay: 0.7          # More persistent impact (reduced from 0.9)
      bid_ask_spread_bps: 5.0
      min_impact_bps: 0.5
      max_impact_bps: 100.0      # Higher impact cap (doubled from 50 bps)
      temporary_impact_decay: 0.5
      enable_bid_ask_bounce: true
  # ADVANCED REWARD SHAPING - Pass configuration to environment
  advanced_reward_config:
    enabled: true                      # Enable advanced reward shaping
    
    # ðŸš€ ENHANCED REWARD SYSTEM - Addresses high-frequency reward noise
    enhanced_reward_system:
      enabled: true                    # âœ… Enable enhanced reward system
      
      # Core reward weights
      realized_pnl_weight: 1.0         # Weight for actual P&L (traditional component)
      directional_weight: 0.3          # Weight for directional signal rewards
      behavioral_weight: 0.2           # Weight for behavioral shaping rewards
      
      # Directional signal parameters (Option A Enhanced)
      directional_scaling: 0.001       # Scale factor for directional rewards
      min_price_change_bps: 0.5        # Minimum price change to reward (0.5 bps)
      
      # Behavioral shaping parameters (Option B Enhanced)
      flip_flop_penalty: 0.001         # Penalty for position changes
      holding_bonus: 0.0001            # Small bonus for maintaining position
      correct_direction_bonus: 0.1     # Bonus for correct directional trades
      wrong_direction_penalty: 0.1     # Penalty for wrong directional trades
      
      # Multi-timeframe aggregation
      enable_multi_timeframe: true     # Enable multi-timeframe rewards
      short_window: 5                  # Short-term window (5 minutes)
      medium_window: 15                # Medium-term window (15 minutes)
      long_window: 60                  # Long-term window (1 hour)
      
      # Adaptive scaling
      enable_adaptive_scaling: true    # Enable adaptive reward scaling
      target_reward_magnitude: 0.01    # Target reward magnitude
      scaling_window: 100              # Window for scaling calculation
      min_scaling_factor: 0.1          # Minimum scaling factor
      max_scaling_factor: 10.0         # Maximum scaling factor
    
    # 1. LAGRANGIAN CONSTRAINT - Learnable multiplier for volatility-based drawdown punishment
    lagrangian_constraint:
      enabled: true                    # Enable Lagrangian constraint learning
      initial_lambda: 0.1              # Initial learnable multiplier Î»
      lambda_lr: 0.001                 # Learning rate for Î» updates
      target_volatility: 0.02          # Target volatility threshold (2% daily)
      vol_window: 60                   # Rolling window for volatility calculation (60 steps = 1h)
      constraint_tolerance: 0.001      # Tolerance for constraint satisfaction
      lambda_min: 0.01                 # Minimum Î» value
      lambda_max: 10.0                 # Maximum Î» value
      update_frequency: 100            # Update Î» every N steps
    
    # 2. SHARPE-ADJUSTED REWARD - Normalize PnL by rolling volatility
    sharpe_adjusted_reward:
      enabled: true                    # Enable Sharpe-adjusted rewards
      rolling_window: 60               # Rolling window for Sharpe calculation (1h)
      min_periods: 30                  # Minimum periods for stable calculation
      sharpe_scaling: 1.0              # Scaling factor for Sharpe rewards
      volatility_floor: 0.001          # Minimum volatility to avoid division by zero
      annualization_factor: 252       # Trading days per year for annualized Sharpe
      
    # 3. CVaR-RL FRAMEWORK - Conditional Value at Risk policy gradient
    cvar_rl:
      enabled: true                    # Enable CVaR-RL for tail risk control
      confidence_level: 0.05           # CVaR confidence level (5% tail risk)
      cvar_window: 120                 # Window for CVaR calculation (2h)
      cvar_weight: 0.3                 # Weight for CVaR term in reward
      tail_penalty_factor: 2.0         # Additional penalty for extreme losses
      quantile_smoothing: 0.1          # Smoothing parameter for quantile estimation
      min_samples_cvar: 50             # Minimum samples before CVaR calculation
evaluation:
  data_duration_for_fetch: 5 D
  metrics:
  - total_return_pct
  - sharpe_ratio
  - max_drawdown_pct
  - num_trades
  - turnover_ratio_period
  - win_rate_pct
  
  # Rolling Window Walk-Forward Backtest Configuration
  rolling_backtest:
    enabled: true                    # Enable rolling window backtest
    training_window_months: 3        # 3-month training windows
    evaluation_window_months: 1      # 1-month evaluation periods
    step_size_months: 1              # Walk forward by 1 month
    min_trading_days: 20             # Minimum trading days per window
    enable_regime_analysis: true     # Analyze performance across market regimes
    save_detailed_results: true      # Save detailed CSV results
    
    # Data range for robustness validation (adjust based on available data)
    data_start_date: "2023-01-01"   # Start date for rolling backtest
    data_end_date: "2024-01-01"     # End date for rolling backtest
    
    # Robustness thresholds for deployment recommendations
    deployment_thresholds:
      excellent_robustness: 0.8      # Threshold for full capital deployment
      good_robustness: 0.6           # Threshold for reduced capital deployment
      fair_robustness: 0.4           # Threshold for paper trading first
      profitable_window_pct: 70      # Minimum % of profitable windows
      max_acceptable_drawdown: 15    # Maximum acceptable drawdown %
feature_engineering:
  # EXTENDED LOOKBACK for temporal pattern recognition with LSTM
  lookback_window: 15  # Extended from 3 to 15 for better temporal patterns
  
  # ENHANCED FEATURE SET with microstructural features
  features:
  - RSI
  - EMA
  - VWAP
  - Time
  - ATR                    # NEW: Average True Range volatility features
  - VWAPRatio             # NEW: Intraday VWAP ratio features  
  - MicroPriceImbalance   # NEW: Micro-price imbalance features
  
  # EXPANDED OBSERVATION FEATURES for LSTM policy
  observation_feature_cols:
  # Traditional technical indicators
  - rsi_14
  - ema_10
  - ema_20
  - vwap
  - hour_sin
  - hour_cos
  # NEW: ATR volatility features
  - atr_14
  - atr_14_normalized
  - atr_regime
  - atr_efficiency
  # NEW: VWAP ratio microstructural features
  - price_vwap_ratio_20
  - vwap_deviation_20
  - vwap_efficiency
  - vwap_reversion_strength
  - vwap_volume_pressure
  # NEW: Micro-price imbalance features
  - order_flow_imbalance_10
  - price_pressure_10
  - volume_adjusted_momentum_10
  - liquidity_proxy
  - market_impact_efficiency
  
  # FEATURE SCALING - expanded for new features
  feature_cols_to_scale:
  # Traditional features
  - rsi_14
  - ema_10
  - ema_20
  - vwap
  - hour_sin
  - hour_cos
  # ATR features
  - atr_14
  - atr_14_normalized
  - atr_efficiency
  # VWAP ratio features
  - price_vwap_ratio_20
  - vwap_deviation_20
  - vwap_efficiency
  - vwap_reversion_strength
  - vwap_volume_pressure
  # Micro-price imbalance features
  - order_flow_imbalance_10
  - price_pressure_10
  - volume_adjusted_momentum_10
  - liquidity_proxy
  - market_impact_efficiency
  
  # FEATURE-SPECIFIC CONFIGURATIONS
  ema:
    windows:
    - 10
    - 20
  rsi:
    window: 14
  time_features:
    include_hour: true
    include_minute: false
    include_day_of_week: false
    include_month: false
    include_quarter: false
    include_year: false
    use_cyclical_encoding: true
  
  # NEW: ATR configuration
  atr:
    windows: [14, 21]           # Multiple ATR periods
    smoothing_method: ema       # Exponential smoothing (Wilder's method)
  
  # NEW: VWAP ratio configuration  
  vwap_ratio:
    windows: [20, 60, 120]      # Multiple VWAP timeframes
    deviation_bands: [1.0, 2.0] # Standard deviation bands
  
  # NEW: Micro-price imbalance configuration
  micro_price_imbalance:
    windows: [5, 10, 20]        # Short-term imbalance windows
    volume_windows: [10, 30]    # Volume analysis windows
    price_impact_window: 5      # Price impact calculation window
logging:
  level: INFO
  log_to_file: true
  log_file_path: logs/orchestrator_gpu_fixed_rainbow_qrdqn.log  # Updated for Rainbow QR-DQN
  max_file_size_mb: 50
  backup_count: 5
monitoring:
  # TensorBoard custom scalars configuration
  tensorboard_frequency: 100        # Log custom scalars every N steps
  enable_custom_scalars: true       # Enable vol_penalty, drawdown_pct, Q_variance, lambda tracking
  
  # Replay buffer audit configuration  
  audit_frequency: 50000           # Audit replay buffer every 50k steps
  audit_sample_size: 1000          # Sample 1k transitions for reward magnitude analysis
  weak_reward_threshold: 0.001     # Threshold for detecting weak reward shaping
  
  # Advanced monitoring features
  track_q_variance: true           # Track Q_max - Q_min spread
  track_lagrangian_lambda: true    # Track Lagrangian multiplier evolution
  track_reward_components: true    # Track individual reward shaping components
  buffer_size: 1000               # Size of metric buffers for smoothing
orchestrator:
  data_dir: data/raw_orch_gpu_rainbow_qrdqn
  feature_dir: data/processed_orch_gpu_rainbow_qrdqn
  model_dir: models/orch_gpu_rainbow_qrdqn
  reports_dir: reports/orch_gpu_rainbow_qrdqn
  run_evaluation: true
  run_training: true
  save_model: true
  save_reports: true
risk:
  # Risk observation features
  include_risk_features: true          # Add risk features to observation space
  vol_window: 60                       # Volatility calculation window (â‰ˆ1h for 1-min data, configurable: 30/90/120 for sweeps)
  penalty_lambda: 1.5                  # INCREASED: Volatility penalty factor (was 0.5)
  target_sigma: 0.0                    # Target volatility threshold - only penalize excess volatility
  dd_limit: 0.03                       # RELAXED: Training drawdown limit (3% vs 2%)
  eval_dd_limit: 0.02                  # STRICT: Evaluation drawdown limit (2%)
  
  # ðŸ”§ FUNCTIONAL REWARD SHAPING - Makes penalties actually apply to rewards!
  reward_shaping:
    enabled: true                      # âœ… Enable functional reward shaping
    penalty_weight: 0.1                # Weight for risk penalties (applied to learning signal)
  
  # Risk policy configuration for reward shaping
  policy_yaml: config/risk_limits.yaml  # Use existing risk limits file
  penalty_weight: 0.1                   # Base penalty weight
  early_stop_threshold: 0.8             # Early stopping threshold
  
  # ADVANCED REWARD & RISK SHAPING - Cutting-edge techniques
  advanced_reward_shaping:
    enabled: true                      # Enable advanced reward shaping
    
    # 1. LAGRANGIAN CONSTRAINT - Learnable multiplier for volatility-based drawdown punishment
    lagrangian_constraint:
      enabled: true                    # Enable Lagrangian constraint learning
      initial_lambda: 0.1              # Initial learnable multiplier Î»
      lambda_lr: 0.001                 # Learning rate for Î» updates
      target_volatility: 0.02          # Target volatility threshold (2% daily)
      vol_window: 60                   # Rolling window for volatility calculation (60 steps = 1h)
      constraint_tolerance: 0.001      # Tolerance for constraint satisfaction
      lambda_min: 0.01                 # Minimum Î» value
      lambda_max: 10.0                 # Maximum Î» value
      update_frequency: 100            # Update Î» every N steps
    
    # 2. SHARPE-ADJUSTED REWARD - Normalize PnL by rolling volatility
    sharpe_adjusted_reward:
      enabled: true                    # Enable Sharpe-adjusted rewards
      rolling_window: 60               # Rolling window for Sharpe calculation (1h)
      min_periods: 30                  # Minimum periods for stable calculation
      sharpe_scaling: 1.0              # Scaling factor for Sharpe rewards
      volatility_floor: 0.001          # Minimum volatility to avoid division by zero
      annualization_factor: 252       # Trading days per year for annualized Sharpe
      
    # 3. CVaR-RL FRAMEWORK - Conditional Value at Risk policy gradient
    cvar_rl:
      enabled: true                    # Enable CVaR-RL for tail risk control
      confidence_level: 0.05           # CVaR confidence level (5% tail risk)
      cvar_window: 120                 # Window for CVaR calculation (2h)
      cvar_weight: 0.3                 # Weight for CVaR term in reward
      tail_penalty_factor: 2.0         # Additional penalty for extreme losses
      quantile_smoothing: 0.1          # Smoothing parameter for quantile estimation
      min_samples_cvar: 50             # Minimum samples before CVaR calculation
  # ADVANCED CURRICULUM LEARNING - Episode-based progression with performance gates
  curriculum:
    enabled: true                      # ENABLE advanced curriculum learning
    gate_check_window: 10              # Episodes to average for gate evaluation
    gate_check_frequency: 5            # Check gates every N episodes
    
    # CURRICULUM STAGES with Performance Gates (AND logic)
    # Stage progression requires BOTH: avg_drawdown < cap*0.75 AND avg_sharpe > -0.5
    stages:
      # Stage 1: Warm-up (Episodes 0-30)
      warm_up:
        episode_start: 0
        episode_end: 30
        drawdown_cap: 0.04             # 4% drawdown cap
        lambda_penalty: 0.5            # Moderate volatility penalty
        min_episodes_in_stage: 10      # Minimum episodes before gate check
        
      # Stage 2: Stabilise (Episodes 31-80)  
      stabilise:
        episode_start: 31
        episode_end: 80
        drawdown_cap: 0.03             # 3% drawdown cap
        lambda_penalty: 1.0            # Increased volatility penalty
        min_episodes_in_stage: 15      # More episodes for stability
        
      # Stage 3: Tighten (Episodes 81-130)
      tighten:
        episode_start: 81
        episode_end: 130
        drawdown_cap: 0.025            # 2.5% drawdown cap
        lambda_penalty: 1.5            # Higher volatility penalty
        min_episodes_in_stage: 15      # Maintain stability requirement
        
      # Stage 4: Final (Episodes 131+)
      final:
        episode_start: 131
        episode_end: null              # Open-ended final stage
        drawdown_cap: 0.02             # 2% drawdown cap (final target)
        lambda_penalty: 2.0            # Maximum volatility penalty
        min_episodes_in_stage: 20      # Extended stability for final stage
risk_management:
  max_daily_drawdown_pct: 0.05
  hourly_turnover_cap: 3.0
  terminate_on_turnover_breach: true   # BINDING: Terminate on excessive turnover
  turnover_penalty_factor: 0.05       # STRONGER: 5% penalty (was 2%)
  turnover_termination_threshold_multiplier: 1.5  # AGGRESSIVE: Terminate at 1.5x cap (was 2x)
  # Enhanced turnover enforcement
  turnover_exponential_penalty_factor: 0.15  # Quadratic penalty factor
  turnover_termination_penalty_pct: 0.08     # 8% portfolio penalty on termination
training:
  algorithm: RecurrentPPO  # UPGRADED: Recurrent PPO with LSTM for temporal pattern recognition
  
  # LSTM POLICY CONFIGURATION for temporal sequence modeling
  policy: MlpLstmPolicy           # LSTM policy for Box observation spaces
  policy_kwargs:                  # Simplified LSTM architecture for stability
    net_arch: [256, 256]          # Smaller Actor-Critic network architecture
    activation_fn: ReLU           # Activation function
    lstm_hidden_size: 64          # Smaller LSTM hidden state size
    n_lstm_layers: 1              # Single LSTM layer
  
  # PPO HYPERPARAMETERS optimized for stability
  learning_rate: 0.0001           # Lower learning rate for stability
  n_steps: 256                    # Smaller steps for faster updates
  batch_size: 32                  # Smaller batch size for memory efficiency
  n_epochs: 4                     # Fewer epochs to reduce training time
  gamma: 0.99                     # Discount factor
  gae_lambda: 0.95                # GAE lambda for advantage estimation
  clip_range: 0.2                 # PPO clipping parameter
  clip_range_vf: null             # Value function clipping (None = same as clip_range)
  normalize_advantage: True       # Normalize advantages
  ent_coef: 0.01                  # Entropy coefficient for exploration
  vf_coef: 0.5                    # Value function coefficient
  max_grad_norm: 0.5              # Gradient clipping
  use_sde: False                  # State-dependent exploration
  sde_sample_freq: -1             # SDE sampling frequency
  target_kl: null                 # Target KL divergence
  
  # EXTENDED TRAINING for temporal pattern learning
  total_timesteps: 50000          # Moderate training for TensorBoard data
  max_episodes: 20                # Fewer episodes but longer training
  max_training_time_minutes: 15   # Longer training time for TensorBoard data
  
  # LSTM-SPECIFIC TRAINING PARAMETERS
  lstm_states_saving: True        # Save LSTM states for evaluation
  sequence_length: 15             # Match lookback_window for temporal consistency
  
  # EARLY STOPPING adapted for PPO
  early_stopping:
    patience: 50                  # Episodes patience for PPO
    min_improvement: 0.02         # Minimum improvement threshold
    check_freq: 5000              # Check every 5k steps
    verbose: true
    min_episodes_before_stopping: 30  # Minimum episodes before early stopping
  
  # MODEL SAVING
  save_freq: 20000                # Save every 20k steps
  save_replay_buffer: false       # PPO doesn't use replay buffer
  tensorboard_log: logs/tensorboard_gpu_recurrent_ppo_microstructural
  verbose: 1
  
  # GPU-SPECIFIC SETTINGS
  device: auto                    # Auto-detect GPU
  gpu_memory_fraction: 0.8        # Use 80% of GPU memory
  mixed_precision: true           # Enable mixed precision for faster training