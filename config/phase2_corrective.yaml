# Phase 2 Corrective Training Configuration
# Fix episode length with carrot-and-smaller-stick approach

ppo:
  # Base parameters
  n_steps:        512           
  batch_size:     128           
  n_epochs:       4             
  gamma:          0.99          
  gae_lambda:     0.95          

  # Learning parameters
  learning_rate:  5e-5          
  clip_range:     0.2           
  ent_coef:       0.03          
  vf_coef:        0.5           
  max_grad_norm:  0.5           
  target_kl:      0.015         

  # Exploration / schedule
  lr_schedule:    constant      
  clip_schedule:  constant      

training:
  total_timesteps: 20000        # Corrective training
  seed:             0           
  envs:             1           
  log_interval:     5           
  save_interval:    1000        
  eval_interval:    1000        
  eval_episodes:    5

# Environment Configuration
environment:
  symbols:          ['NVDA', 'MSFT']
  initial_capital:  10000.0
  lookback_window:  50
  max_episode_steps: 390        
  transaction_cost_pct: 0.001
  max_drawdown_pct: 0.60        # Start at 60%, will drop to 55% at 5K

  # Data configuration
  start_date: '2022-01-03'
  end_date: '2024-01-31'
  bar_size: '2min'
  data_split: 'train'

# Corrective Reward System - Carrot and Smaller Stick
reward_system:
  type: 'refined_wrapper'  # Use RefinedRewardSystem wrapper
  
  # RefinedRewardSystem parameters
  parameters:
    pnl_epsilon: 750.0
    holding_alpha: 0.05
    penalty_beta: 0.1
    exploration_coef: 0.05
    
    # Corrective incentive structure
    early_exit_tax: 3.0         # Reduced from 5.0 (smaller stick)
    min_episode_length: 70      # Threshold for early-exit tax
    
    # NEW: Positive incentives (carrots)
    time_bonus: 0.02            # +0.02 per step ≥ 60
    time_bonus_threshold: 60    # Start time bonus at 60 steps
    completion_bonus: 2.0       # +2.0 bonus for episodes ≥ 80 steps
    completion_threshold: 80    # Threshold for completion bonus

# Corrective curriculum schedule
curriculum:
  # Corrective phase: 0 → 20K steps with incentive rebalancing
  corrective:
    steps: [0, 20000]
    drawdown_schedule:
      - step: 0
        value: 0.60
      - step: 5000  
        value: 0.55
    learning_rate: 5e-5
    target_kl: 0.015
    goal: "Fix episode length with positive incentives"