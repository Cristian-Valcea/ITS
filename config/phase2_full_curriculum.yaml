# Phase 2 - Full Curriculum Configuration
# 100K steps: Warm-keep (75% DD) → Risk-tighten (60% DD) → Profit-polish (adaptive LR)

ppo:
  # Base parameters - will be overridden by command line
  n_steps:        512           
  batch_size:     128           
  n_epochs:       4             
  gamma:          0.99          
  gae_lambda:     0.95          

  # Starting parameters (will adapt)
  learning_rate:  5e-5          
  clip_range:     0.2           
  ent_coef:       0.03          
  vf_coef:        0.5           
  max_grad_norm:  0.5           
  target_kl:      0.015         # Will change to 0.012 in profit-polish

  # Exploration / schedule
  lr_schedule:    constant      
  clip_schedule:  constant      

training:
  total_timesteps: 100000       # Full curriculum
  seed:             0           # Fixed seed for consistency
  envs:             1           
  log_interval:     5           
  save_interval:    2500        
  eval_interval:    2500        
  eval_episodes:    5

# Environment Configuration - starts at 75% DD
environment:
  symbols:          ['NVDA', 'MSFT']
  initial_capital:  10000.0
  lookback_window:  50
  max_episode_steps: 390        
  transaction_cost_pct: 0.001
  max_drawdown_pct: 0.75        # Will transition to 0.60 at 20K steps

  # Data configuration
  start_date: '2022-01-03'
  end_date: '2024-01-31'
  bar_size: '2min'
  data_split: 'train'

# Reward system configuration with early-exit tax
reward_system:
  type: 'refined_wrapper'  # Use RefinedRewardSystem wrapper
  
  # RefinedRewardSystem parameters
  parameters:
    pnl_epsilon: 750.0
    holding_alpha: 0.05
    penalty_beta: 0.1
    exploration_coef: 0.05
    
    # Early-exit tax parameters (proven effective in pilot)
    early_exit_tax: 5.0         # Penalty for episodes < 80 steps
    min_episode_length: 80      # Threshold for early-exit tax

# Curriculum schedule (implemented via command line parameters)
curriculum:
  # Warm-keep: 0 → 20K steps @ 75% DD
  warm_keep:
    steps: [0, 20000]
    drawdown_pct: 0.75
    learning_rate: 5e-5
    target_kl: 0.015
    goal: "Stabilise reward trend"
    
  # Risk-tighten: 20K → 60K steps @ 60% DD  
  risk_tighten:
    steps: [20000, 60000]
    drawdown_pct: 0.60
    learning_rate: 5e-5
    target_kl: 0.015
    goal: "Teach to thrive with stricter DD"
    
  # Profit-polish: 60K → 100K steps @ 60% DD + adaptive LR
  profit_polish:
    steps: [60000, 100000]
    drawdown_pct: 0.60
    learning_rate: "adaptive"  # 5e-5 → 2e-5 if plateau
    target_kl: 0.012
    goal: "Push mean reward ≥ -5"