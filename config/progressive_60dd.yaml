# Progressive Training Config - Phase 2/3: 60% Drawdown
# Based on progressive_75dd.yaml but with 60% drawdown limit

ppo:
  # Same parameters as progressive_75dd for consistency
  n_steps:        512           
  batch_size:     128           
  n_epochs:       4             
  gamma:          0.99          
  gae_lambda:     0.95          

  # Optimization - will be overridden by command line
  learning_rate:  5e-5          # G5 optimal
  clip_range:     0.2           
  ent_coef:       0.03          
  vf_coef:        0.5           
  max_grad_norm:  0.5           
  target_kl:      0.015         # G5 optimal (Phase 2) / 0.012 (Phase 3)

  # Exploration / schedule
  lr_schedule:    constant      
  clip_schedule:  constant      

training:
  total_timesteps: 30000        # Will be overridden by script
  seed:             0           # Fixed seed for consistency
  envs:             1           
  log_interval:     5           
  save_interval:    10          
  eval_interval:    10          
  eval_episodes:    5

# Environment Configuration
environment:
  symbols:          ['NVDA', 'MSFT']
  initial_capital:  10000.0
  lookback_window:  50
  max_episode_steps: 390        
  transaction_cost_pct: 0.001
  max_drawdown_pct: 0.60        # 60% DD cap for curriculum/profit phases

  # Data configuration
  start_date: '2022-01-03'
  end_date: '2024-01-31'
  bar_size: '2min'
  data_split: 'train'

# Reward system configuration
reward_system:
  type: 'refined_wrapper'  # Use RefinedRewardSystem wrapper
  
  # RefinedRewardSystem parameters
  parameters:
    pnl_epsilon: 750.0
    holding_alpha: 0.05
    penalty_beta: 0.1
    exploration_coef: 0.05