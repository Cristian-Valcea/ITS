# PHASE 1: REALITY GROUNDING FOUNDATION
# Objective: Fix reward scaling disconnection while maintaining system stability
# Expected Impact: Episode rewards 8k-19k (realistic scale), proper observation consistency

environment:
  initial_capital: 50000.0
  reward_scaling: 0.07  # ðŸ”§ FINAL-CALIBRATION: Target ep_rew_mean 4-6 band
  
  # Institutional safeguards
  max_position_size_pct: 0.95  # Never go more than 95% long/short
  min_cash_reserve_pct: 0.05   # Always maintain 5% cash buffer
  
  # EarlyStoppingCallback threshold scaling (addressing team feedback)
  early_stopping:
    patience: 20
    min_delta: 100  # Scaled from 5 to match new reward scale (19k * 0.005 â‰ˆ 100)
    plateau_threshold: 500  # 19k * 0.025 = 475, rounded to 500
    
risk:
  include_risk_features: true
  soft_dd_pct: 0.02    # 2.0% soft limit - penalties applied
  hard_dd_pct: 0.04    # 4.0% hard limit - ignored in Phase 1
  penalty_lambda: 2500.0  # Static fallback - overridden by dynamic schedule
  # Dynamic penalty lambda schedule
  dynamic_lambda_schedule: true
  lambda_start: 300.0     # ðŸ”„ PHASE1-FIX: lower entry penalty
  lambda_end: 3000.0      # ðŸ”„ PHASE1-FIX: keep 10Ã— span but cut absolute max
  lambda_schedule_steps: 25000  # Linear increase over 25k steps
  
  # DD baseline reset mechanism
  dd_baseline_reset_enabled: true
  dd_recovery_threshold_pct: 0.005  # +0.5% equity recovery threshold
  dd_reset_timeout_steps: 800       # Reset baseline after 800 steps regardless
  
  # Positive recovery bonus
  recovery_bonus_enabled: true
  recovery_bonus_amount: 0.2        # +0.2 reward every step when above baseline
  
  # Early-warning logger
  early_warning_enabled: true
  early_warning_threshold_pct: 0.005  # 0.5% excess DD threshold for warning
  early_warning_duration_steps: 50    # Warn if above threshold for > 50 steps
  terminate_on_hard: false  # Phase 1: No termination, just penalties
  
  # Enhanced risk feature set (institutional standard)
  risk_features:
    - portfolio_heat_ratio      # Current risk / Risk budget
    - concentration_ratio       # Single position concentration  
    - drawdown_velocity         # Rate of drawdown acceleration
    - var_breach_indicator      # Boolean: Are we breaching VaR?
    - correlation_breakdown     # Correlation regime shift detector
    
validation:
  observation_consistency_check: true  # Ensure train/eval identical
  reward_bounds_check: true           # Alert on extreme rewards
  nan_guard_strict: true              # Zero tolerance for NaN values
  
  # Batch sanity test (addressing team feedback)
  consistency_test:
    sample_size: 128
    tolerance: 1e-6
    test_frequency: "every_1000_steps"
    
  # Reward bounds for institutional validation (Step 3: Tighter clipping)
  reward_bounds:
    min_reward: -150  # Step 3: Clip at Â±150 raw (Â±37.5 scaled)
    max_reward: 150   # Prevent rare spikes from exploding gradients
    alert_threshold: 0.95  # Alert if within 5% of bounds

# Model compatibility validation
model_validation:
  enforce_compatibility: true
  expected_observation_features: 11  # 6 base + 5 risk features
  check_frequency: "initialization"
  
# Logging and monitoring
logging:
  level: "INFO"
  reward_scaling_logs: true
  consistency_test_logs: true
  safeguard_violation_logs: true
  
# TensorBoard configuration
tensorboard:
  log_dir: "logs/tensorboard_phase1"
  update_freq: 100
  write_graph: true
  write_images: false
  
# Training configuration (Step 2: Enhanced PPO params)
training:
  algorithm: RecurrentPPO
  policy: MlpLstmPolicy
  total_timesteps: 100000  # Step 5: Doubling schedule compensates initial stalls
  tensorboard_log: "logs/tensorboard_phase1_fix1"
  
  # Step 2: Help critic learn bigger variance
  normalize_advantage: true  # Normalize advantages for better learning
  vf_coef: 0.8              # Increase value function coefficient
  
# Phase 1 success criteria
success_criteria:
  episode_reward_range: [8000, 19000]
  entropy_floor: -0.25
  explained_variance_threshold: 0.85
  observation_consistency_rate: 0.99
  nan_incidents_tolerance: 0

# ---------------------------------------------------------------------
# ðŸ”„ PHASE1-FIX 2025-07-22
# Changes for reward-penalty rebalance
#   â€¢ reward_scaling: 0.1 â†’ 0.3   (3Ã— reward amplitude)
#   â€¢ lambda_start  : 1000 â†’ 300  (â‰ˆ3.3Ã— softer)
#   â€¢ lambda_end    : 10000 â†’ 3000(â‰ˆ3.3Ã— softer high cap)
# Expect penalties â‰¤25 % of gross reward, ep_rew_mean â‰¥--trend upward to â‰¥15 by 15 k steps.
# ---------------------------------------------------------------------