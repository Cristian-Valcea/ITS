# config/model_params.yaml
# Configuration for RL algorithm selection and hyperparameters.
# TrainerAgent will use this.

# --- Algorithm Selection ---
algorithm_name: "RecurrentPPO"  # Options: "DQN", "C51" (if available), "PPO", "A2C", "RecurrentPPO", etc.
                                 # Ensure the chosen algorithm is compatible with Stable-Baselines3
                                 # and the environment's action space (Discrete for this setup).

# --- Common Algorithm Parameters (for SB3 models like DQN, PPO, A2C) ---
# These are general parameters; some might not apply to all algorithms.
# Refer to Stable-Baselines3 documentation for specific algorithm parameters.
algorithm_params:
  policy: "MlpLstmPolicy"          # Policy network type for RecurrentPPO. Examples:
                                   # "MlpPolicy" (for flat/vector features)
                                   # "MlpLstmPolicy" (for sequential/temporal data with LSTM)
                                   # "MultiInputLstmPolicy" (for Dict observation spaces with LSTM)
                                   # "CnnPolicy" (for image-like features, if features are shaped like images)
                                   # Custom policies can also be specified.
  
  learning_rate: 0.0003 # Learning rate for the optimizer (higher for PPO)
  
  gamma: 0.99           # Discount factor for future rewards.
  
  verbose: 1            # Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages.
  
  seed: null            # Seed for the pseudo random generators to ensure reproducibility.
                        # If null, uses global_seed from main_config or a random one.

  # --- Parameters specific to PPO/RecurrentPPO (On-Policy) ---
  n_steps: 2048         # Number of steps to run for each environment per update
  batch_size: 256       # Minibatch size for each gradient update
  n_epochs: 10          # Number of epochs when optimizing the surrogate loss
  gae_lambda: 0.95      # Factor for trade-off of bias vs variance for Generalized Advantage Estimator
  clip_range: 0.2       # Clipping parameter for PPO
  ent_coef: 0.01        # Entropy coefficient for the loss calculation
  vf_coef: 0.5          # Value function coefficient for the loss calculation
  max_grad_norm: 0.5    # Maximum value for the gradient clipping
  
  # LSTM-specific parameters for RecurrentPPO
  policy_kwargs:
    net_arch: [512, 512]          # Actor-Critic network architecture
    activation_fn: ReLU           # Activation function
    lstm_hidden_size: 256         # LSTM hidden state size for temporal memory
    n_lstm_layers: 2              # Number of LSTM layers for deep temporal modeling
    shared_lstm: False            # Separate LSTM for actor and critic
    enable_critic_lstm: True      # Enable LSTM in critic network


