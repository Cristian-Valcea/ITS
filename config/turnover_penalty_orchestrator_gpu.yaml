# ðŸŽ¯ TURNOVER PENALTY SYSTEM Configuration
# This configuration enables the new normalized turnover penalty system
# Based on emergency_fix_orchestrator_gpu.yaml but with improved turnover controls

data_augmentation:
  noise_injection: false
  noise_level: 0.02
  random_scaling: false
  scale_max: 1.02
  scale_min: 0.98
  window_size: 60
  window_slicing: false

ibkr_conn:
  host: 127.0.0.1
  port: 7497
  clientId: 103
  timeout: 30
  use_cache: true
  cache_duration_minutes: 5
  simulation_mode: true

data_preprocessing:
  impute_missing: true
  normalize: false

feature_store:
  force_duckdb: true

environment:
  initial_capital: 50000.0
  log_trades_in_env: true
  position_sizing_pct_capital: 0.25
  equity_scaling_factor: 0.02
  reward_scaling: 1.0
  trade_cooldown_steps: 0
  max_episode_steps: 20000
  
  # ðŸŽ¯ FIXED TURNOVER PENALTY SYSTEM - ENABLED
  use_turnover_penalty: true
  turnover_target_ratio: 0.02        # Target 2% turnover ratio (dimensionless)
  turnover_weight_factor: 0.15       # Penalty weight: 15% of NAV (strong deterrent)
  turnover_curve_sharpness: 25.0     # Sigmoid curve sharpness (k=25)
  turnover_penalty_type: quadratic   # Use quadratic penalty for aggressive overtrading control
  
  # ðŸš¨ DISABLE EMERGENCY FIX - Use new system instead
  use_emergency_reward_fix: false
  emergency_transaction_cost_pct: 0.0003  # Realistic exchange + slippage costs
  emergency_holding_bonus: 0.0           # No holding bonus needed
  
  # Realistic exchange fees + slippage
  transaction_cost_pct: 0.0003  # 0.03% (3 basis points) - realistic exchange + slippage
  
  # Disable old turnover controls
  action_change_penalty_factor: 0.0
  turnover_bonus_threshold: 0.0
  turnover_bonus_factor: 0.0
  
  # Enable Kyle Lambda for realistic market impact
  kyle_lambda_fills:
    enable_kyle_lambda_fills: true
    
  # Enable advanced reward shaping
  advanced_reward_config:
    enabled: true
    
    enhanced_reward_system:
      enabled: true
      
    lagrangian_constraint:
      enabled: false
      
    sharpe_adjusted_reward:
      enabled: false
      
    cvar_rl:
      enabled: false

evaluation:
  data_duration_for_fetch: 5 D
  metrics:
  - total_return_pct
  - sharpe_ratio
  - max_drawdown_pct
  - num_trades
  - turnover_ratio_period
  - win_rate_pct
  
  rolling_backtest:
    enabled: false

feature_engineering:
  lookback_window: 5
  
  features:
  - RSI
  - EMA
  - VWAP
  - Time
  
  observation_feature_cols:
  - rsi_14
  - ema_10
  - ema_20
  - vwap
  - hour_sin
  - hour_cos
  
  feature_cols_to_scale:
  - rsi_14
  - ema_10
  - ema_20
  - vwap
  - hour_sin
  - hour_cos
  
  ema:
    windows:
    - 10
    - 20
  rsi:
    window: 14
  time_features:
    include_hour: true
    include_minute: false
    include_day_of_week: false
    include_month: false
    include_quarter: false
    include_year: false
    use_cyclical_encoding: true

logging:
  level: INFO
  log_to_file: true
  log_file_path: logs/turnover_penalty_orchestrator_gpu.log
  max_file_size_mb: 50
  backup_count: 5

monitoring:
  tensorboard_frequency: 100
  enable_custom_scalars: true
  audit_frequency: 10000
  audit_sample_size: 500
  weak_reward_threshold: 0.001
  track_q_variance: true
  track_lagrangian_lambda: false
  track_reward_components: true
  buffer_size: 1000

orchestrator:
  data_dir: data/raw_turnover_penalty
  feature_dir: data/processed_turnover_penalty
  model_dir: models/turnover_penalty
  reports_dir: reports/turnover_penalty
  run_evaluation: true
  run_training: true
  save_model: true
  save_reports: true

risk:
  include_risk_features: true
  vol_window: 60
  penalty_lambda: 0.05
  target_sigma: 0.015
  dd_limit: 0.05
  eval_dd_limit: 0.03
  
  reward_shaping:
    enabled: false
    
  advanced_reward_shaping:
    enabled: false

risk_management:
  max_daily_drawdown_pct: 0.05
  # Standard turnover controls (not used with new system)
  hourly_turnover_cap: 3.0
  terminate_on_turnover_breach: false
  turnover_penalty_factor: 0.0  # Disabled - using new system
  turnover_termination_threshold_multiplier: 3.0
  turnover_exponential_penalty_factor: 0.0  # Disabled - using new system
  turnover_termination_penalty_pct: 0.0     # Disabled - using new system

training:
  algorithm: RecurrentPPO
  
  policy: MlpLstmPolicy
  policy_kwargs:
    net_arch: [64, 64]
    activation_fn: ReLU
    lstm_hidden_size: 32
    n_lstm_layers: 1
  
  learning_rate: 0.0001
  n_steps: 128
  batch_size: 32
  n_epochs: 4
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  normalize_advantage: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  sde_sample_freq: -1
  target_kl: null
  
  total_timesteps: 100000
  max_episodes: 80
  max_training_time_minutes: 90
  
  lstm_states_saving: True
  sequence_length: 5
  
  early_stopping:
    patience: 40
    min_improvement: 0.001
    check_freq: 2000
    verbose: true
    min_episodes_before_stopping: 25
  
  save_freq: 5000
  save_replay_buffer: false
  tensorboard_log: logs/tensorboard_turnover_penalty
  verbose: 1
  
  device: auto
  gpu_memory_fraction: 0.6
  mixed_precision: false

# ðŸŽ¯ FIXED TURNOVER PENALTY SYSTEM VALIDATION CRITERIA
turnover_penalty_validation:
  target_metrics:
    turnover_ratio: 0.025           # Target: 2.5% turnover ratio (dimensionless)
    min_sharpe_ratio: 0.5           # Should be > 0.5
    max_drawdown_pct: 0.08          # Should be < 8%
    min_win_rate: 0.50              # Should be > 50%
    min_total_return: 0.02          # Should be > 2%
    penalty_range: [-1000, -200]    # Expected penalty range for $50k portfolio
  
  success_criteria:
    - "Turnover ratio stays near 2% target (dimensionless, â‰¤1 for 1Ã— capital)"
    - "Penalty scales with NAV (2% of portfolio value)"
    - "Smooth sigmoid curve prevents cliff effects"
    - "Model trades selectively for profit"
    - "Sharpe ratio improves significantly"
    - "No episode length dependency"