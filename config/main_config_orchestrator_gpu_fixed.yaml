data_augmentation:
  noise_injection: true
  noise_level: 0.02
  random_scaling: true
  scale_max: 1.02
  scale_min: 0.98
  window_size: 60
  window_slicing: false
ibkr_conn:
  host: 127.0.0.1
  port: 7497
  clientId: 103
  timeout: 30
  use_cache: true
  cache_duration_minutes: 5
  simulation_mode: true
data_preprocessing:
  impute_missing: true
  normalize: false
feature_store:
  force_duckdb: true  # Skip PostgreSQL for training jobs
environment:
  initial_capital: 50000.0
  log_trades_in_env: true
  position_sizing_pct_capital: 0.25  # Legacy percentage-based sizing
  equity_scaling_factor: 0.02       # k factor for equity-scaled sizing: shares = k * portfolio_value / price
  reward_scaling: 0.01    # Scale rewards for optimal gradient signals (100x increase from 0.0001)
  trade_cooldown_steps: 12  # STRONG THROTTLE: Require 12 bars (12 minutes) between trades
  transaction_cost_pct: 0.001
  action_change_penalty_factor: 0.001  # L2 penalty for action changes to discourage ping-ponging
  # Reward shaping parameters
  turnover_bonus_threshold: 0.8        # Bonus when turnover < 80% of cap
  turnover_bonus_factor: 0.001         # Bonus amount per step when under threshold
  # Enhanced Kyle Lambda fill simulation for realistic market impact
  kyle_lambda_fills:
    enable_kyle_lambda_fills: true
    fill_simulator_config:
      lookback_period: 50
      min_periods: 10
      impact_decay: 0.7          # More persistent impact (reduced from 0.9)
      bid_ask_spread_bps: 5.0
      min_impact_bps: 0.5
      max_impact_bps: 100.0      # Higher impact cap (doubled from 50 bps)
      temporary_impact_decay: 0.5
      enable_bid_ask_bounce: true
  # ADVANCED REWARD SHAPING - Pass configuration to environment
  advanced_reward_config:
    enabled: true                      # Enable advanced reward shaping
    
    # 1. LAGRANGIAN CONSTRAINT - Learnable multiplier for volatility-based drawdown punishment
    lagrangian_constraint:
      enabled: true                    # Enable Lagrangian constraint learning
      initial_lambda: 0.1              # Initial learnable multiplier λ
      lambda_lr: 0.001                 # Learning rate for λ updates
      target_volatility: 0.02          # Target volatility threshold (2% daily)
      vol_window: 60                   # Rolling window for volatility calculation (60 steps = 1h)
      constraint_tolerance: 0.001      # Tolerance for constraint satisfaction
      lambda_min: 0.01                 # Minimum λ value
      lambda_max: 10.0                 # Maximum λ value
      update_frequency: 100            # Update λ every N steps
    
    # 2. SHARPE-ADJUSTED REWARD - Normalize PnL by rolling volatility
    sharpe_adjusted_reward:
      enabled: true                    # Enable Sharpe-adjusted rewards
      rolling_window: 60               # Rolling window for Sharpe calculation (1h)
      min_periods: 30                  # Minimum periods for stable calculation
      sharpe_scaling: 1.0              # Scaling factor for Sharpe rewards
      volatility_floor: 0.001          # Minimum volatility to avoid division by zero
      annualization_factor: 252       # Trading days per year for annualized Sharpe
      
    # 3. CVaR-RL FRAMEWORK - Conditional Value at Risk policy gradient
    cvar_rl:
      enabled: true                    # Enable CVaR-RL for tail risk control
      confidence_level: 0.05           # CVaR confidence level (5% tail risk)
      cvar_window: 120                 # Window for CVaR calculation (2h)
      cvar_weight: 0.3                 # Weight for CVaR term in reward
      tail_penalty_factor: 2.0         # Additional penalty for extreme losses
      quantile_smoothing: 0.1          # Smoothing parameter for quantile estimation
      min_samples_cvar: 50             # Minimum samples before CVaR calculation
evaluation:
  data_duration_for_fetch: 5 D
  metrics:
  - total_return_pct
  - sharpe_ratio
  - max_drawdown_pct
  - num_trades
  - turnover_ratio_period
  - win_rate_pct
feature_engineering:
  ema:
    windows:
    - 10
    - 20
  feature_cols_to_scale:
  - rsi_14
  - ema_10
  - ema_20
  - vwap
  - hour_sin
  - hour_cos
  features:
  - RSI
  - EMA
  - VWAP
  - Time
  lookback_window: 3
  observation_feature_cols:
  - rsi_14
  - ema_10
  - ema_20
  - vwap
  - hour_sin
  - hour_cos
  rsi:
    window: 14
  time_features:
    include_hour: true
    include_minute: false
    include_day_of_week: false
    include_month: false
    include_quarter: false
    include_year: false
    use_cyclical_encoding: true
logging:
  level: INFO
  log_to_file: true
  log_file_path: logs/orchestrator_gpu_fixed_rainbow_qrdqn.log  # Updated for Rainbow QR-DQN
  max_file_size_mb: 50
  backup_count: 5
orchestrator:
  data_dir: data/raw_orch_gpu_rainbow_qrdqn
  feature_dir: data/processed_orch_gpu_rainbow_qrdqn
  model_dir: models/orch_gpu_rainbow_qrdqn
  reports_dir: reports/orch_gpu_rainbow_qrdqn
  run_evaluation: true
  run_training: true
  save_model: true
  save_reports: true
risk:
  # Risk observation features
  include_risk_features: true          # Add risk features to observation space
  vol_window: 60                       # Volatility calculation window (≈1h for 1-min data, configurable: 30/90/120 for sweeps)
  penalty_lambda: 1.5                  # INCREASED: Volatility penalty factor (was 0.5)
  target_sigma: 0.0                    # Target volatility threshold - only penalize excess volatility
  dd_limit: 0.03                       # RELAXED: Training drawdown limit (3% vs 2%)
  eval_dd_limit: 0.02                  # STRICT: Evaluation drawdown limit (2%)
  
  # ADVANCED REWARD & RISK SHAPING - Cutting-edge techniques
  advanced_reward_shaping:
    enabled: true                      # Enable advanced reward shaping
    
    # 1. LAGRANGIAN CONSTRAINT - Learnable multiplier for volatility-based drawdown punishment
    lagrangian_constraint:
      enabled: true                    # Enable Lagrangian constraint learning
      initial_lambda: 0.1              # Initial learnable multiplier λ
      lambda_lr: 0.001                 # Learning rate for λ updates
      target_volatility: 0.02          # Target volatility threshold (2% daily)
      vol_window: 60                   # Rolling window for volatility calculation (60 steps = 1h)
      constraint_tolerance: 0.001      # Tolerance for constraint satisfaction
      lambda_min: 0.01                 # Minimum λ value
      lambda_max: 10.0                 # Maximum λ value
      update_frequency: 100            # Update λ every N steps
    
    # 2. SHARPE-ADJUSTED REWARD - Normalize PnL by rolling volatility
    sharpe_adjusted_reward:
      enabled: true                    # Enable Sharpe-adjusted rewards
      rolling_window: 60               # Rolling window for Sharpe calculation (1h)
      min_periods: 30                  # Minimum periods for stable calculation
      sharpe_scaling: 1.0              # Scaling factor for Sharpe rewards
      volatility_floor: 0.001          # Minimum volatility to avoid division by zero
      annualization_factor: 252       # Trading days per year for annualized Sharpe
      
    # 3. CVaR-RL FRAMEWORK - Conditional Value at Risk policy gradient
    cvar_rl:
      enabled: true                    # Enable CVaR-RL for tail risk control
      confidence_level: 0.05           # CVaR confidence level (5% tail risk)
      cvar_window: 120                 # Window for CVaR calculation (2h)
      cvar_weight: 0.3                 # Weight for CVaR term in reward
      tail_penalty_factor: 2.0         # Additional penalty for extreme losses
      quantile_smoothing: 0.1          # Smoothing parameter for quantile estimation
      min_samples_cvar: 50             # Minimum samples before CVaR calculation
  # Curriculum learning - ENABLED for progressive risk tightening
  curriculum:
    enabled: true                      # ENABLE curriculum learning
    logic: and                         # Logic for curriculum triggers
    use_perf_trigger: false           # Don't use performance triggers
    stages:
      - steps: 0
        dd_limit: 0.04
        lambda: 0.5
      - steps: 50000
        dd_limit: 0.03
        lambda: 1.0
      - steps: 150000
        dd_limit: 0.025
        lambda: 1.5
      - steps: 300000
        dd_limit: 0.02
        lambda: 2.0
risk_management:
  max_daily_drawdown_pct: 0.05
  hourly_turnover_cap: 3.0
  terminate_on_turnover_breach: true   # BINDING: Terminate on excessive turnover
  turnover_penalty_factor: 0.05       # STRONGER: 5% penalty (was 2%)
  turnover_termination_threshold_multiplier: 1.5  # AGGRESSIVE: Terminate at 1.5x cap (was 2x)
  # Enhanced turnover enforcement
  turnover_exponential_penalty_factor: 0.15  # Quadratic penalty factor
  turnover_termination_penalty_pct: 0.08     # 8% portfolio penalty on termination
training:
  algorithm: QR-DQN  # UPGRADED: Quantile Regression DQN (Distributional RL)
  # Advanced DQN Configuration - Rainbow DQN Components
  # 1. Double DQN: Built into SB3 (target network reduces overestimation bias)
  # 2. Distributional RL: Learn full return distribution instead of expected value
  # 3. Enhanced Architecture: Larger network capacity for complex learning
  policy: MultiInputPolicy    # Base policy for multi-input observations
  policy_kwargs:             # Enhanced network architecture for distributional learning
    net_arch: [512, 512, 256]  # Larger network for complex feature learning
    activation_fn: ReLU      # Activation function
    n_quantiles: 200         # DISTRIBUTIONAL: Number of quantiles for return distribution
  
  # Enhanced exploration with epsilon-greedy (optimized for distributional learning)
  exploration_fraction: 0.3   # FASTER DECAY: Distributional RL learns faster
  exploration_final_eps: 0.02 # LOWER FINAL: More exploitation with better value estimates
  exploration_initial_eps: 0.8 # HIGHER START: More exploration for distributional learning
  # GPU-Optimized hyperparameters with improved learning
  buffer_size: 500000  # Increased for GPU memory (was 200k)
  batch_size: 512      # Larger batches for GPU efficiency (was 256)
  exploration_fraction: 0.4   # SLOWER DECAY: Extended exploration period for conservative learning
  exploration_final_eps: 0.05  # MODERATE FINAL: Balanced exploration/exploitation
  exploration_initial_eps: 0.5 # TAMED START: Start with moderate exploration (was 1.0)
  gamma: 0.99
  gradient_steps: 1
  learning_rate: 0.0001
  learning_starts: 1000
  target_update_interval: 1000
  train_freq: 4
  # Extended training for GPU with better convergence
  total_timesteps: 500000  # Increased from 300k for GPU power
  max_episodes: 200  # Allow many episodes for meaningful training
  max_training_time_minutes: 15  # At least 15 minutes of real training time
  # Improved early stopping for learning - step-based checking
  early_stopping:
    patience: 100  # Much more patience for learning convergence (episodes)
    min_improvement: 0.01  # Require meaningful improvement
    check_freq: 10000  # Check every 10k steps for stability
    verbose: true
    min_episodes_before_stopping: 50  # Minimum episodes before early stopping kicks in
  # Model saving
  save_freq: 10000
  save_replay_buffer: false
  tensorboard_log: logs/tensorboard_gpu_rainbow_qrdqn
  verbose: 1
  # GPU-specific settings
  device: auto  # Will auto-detect GPU
  gpu_memory_fraction: 0.8  # Use 80% of GPU memory
  mixed_precision: true  # Enable mixed precision for faster training