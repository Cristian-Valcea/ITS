#!/usr/bin/env python3
"""
Large Dataset Lazy Loading Benchmark
Demonstrates memory efficiency for realistic production scenarios
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
import time
import psutil
import logging

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'src'))

from data.lazy_data_loader import LazyDataLoader, ChunkInfo

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ProductionScenarioBenchmark:
    """
    Realistic production scenario benchmark
    Simulates processing 1 year of minute-level data for multiple symbols
    """
    
    def __init__(self):
        self.process = psutil.Process()
        
    def simulate_large_lazy_processing(self):
        """Simulate processing large dataset with lazy loading - only load what you need"""
        
        print("üîÑ LAZY LOADING: Processing 1 year of minute data (selective access)")
        print("=" * 60)
        
        start_memory = self.process.memory_info().rss / 1024 / 1024
        start_time = time.time()
        
        # Simulate 1 year of minute data split into monthly chunks
        # 252 trading days * 390 minutes/day = 98,280 minutes per symbol
        # With 12 features per symbol, each symbol ‚âà 15 MB/month
        
        monthly_chunks = []
        for month in range(12):  # 12 months
            chunk_size_mb = np.random.uniform(14, 16)  # Realistic variation
            chunk = ChunkInfo(
                chunk_id=month,
                start_idx=month * 8190,
                end_idx=(month + 1) * 8190,
                size_mb=chunk_size_mb,
                timestamp_range=(f"2024-{month+1:02d}-01", f"2024-{month+1:02d}-28")
            )
            monthly_chunks.append(chunk)
        
        loader = LazyDataLoader(chunk_size_mb=20.0, cache_size_mb=50.0)
        
        print(f"üìä Dataset: 12 monthly chunks, ~{sum(c.size_mb for c in monthly_chunks):.1f} MB total")
        print(f"üéØ Strategy: Only load chunks needed for analysis")
        print(f"üíæ Initial memory: {start_memory:.1f} MB")
        
        # Realistic scenario: Only need to analyze Q4 data (last 3 months)
        q4_chunks = monthly_chunks[-3:]  # Only Oct, Nov, Dec
        
        processed_data = []
        memory_samples = []
        
        for i, chunk in enumerate(q4_chunks):
            print(f"   üìñ Loading Q4 month {i+1}/3: {chunk.timestamp_range[0]} ({chunk.size_mb:.1f} MB)")
            
            # Load chunk
            chunk_data = loader.load_chunk(chunk)
            
            # Process the data (e.g., calculate indicators, run model)
            processed_chunk = self._process_chunk_data(chunk_data)
            processed_data.append(processed_chunk)
            
            current_memory = self.process.memory_info().rss / 1024 / 1024
            memory_samples.append(current_memory)
            
            print(f"   ‚úÖ Processed {len(chunk_data):,} rows ‚Üí {len(processed_chunk):,} signals")
            print(f"   üíæ Current memory: {current_memory:.1f} MB")
        
        end_time = time.time()
        peak_memory = max(memory_samples)
        total_time = end_time - start_time
        
        # Only loaded 3 months instead of 12 months
        data_efficiency = (3 / 12) * 100  # 25% of total dataset
        
        print(f"\nüìà LAZY LOADING RESULTS:")
        print(f"   ‚úÖ Selective loading: 3/12 months ({data_efficiency:.0f}% of dataset)")
        print(f"   üìä Data processed: {sum(c.size_mb for c in q4_chunks):.1f} MB")
        print(f"   üíæ Peak memory: {peak_memory:.1f} MB")
        print(f"   ‚è±Ô∏è Processing time: {total_time:.2f} seconds")
        print(f"   üéØ Memory efficiency: Only needed data loaded")
        
        return {
            'peak_memory_mb': peak_memory,
            'memory_increase_mb': peak_memory - start_memory,
            'data_loaded_mb': sum(c.size_mb for c in q4_chunks),
            'processing_time_sec': total_time,
            'data_efficiency_pct': data_efficiency
        }
    
    def simulate_traditional_processing(self):
        """Simulate traditional approach - load all data upfront"""
        
        print(f"\nüîÑ TRADITIONAL LOADING: Load entire year upfront")
        print("=" * 60)
        
        start_memory = self.process.memory_info().rss / 1024 / 1024
        start_time = time.time()
        
        print(f"üíæ Initial memory: {start_memory:.1f} MB")
        print(f"üìñ Loading entire year of data at once...")
        
        # Simulate loading entire year (12 months * 15MB/month = 180MB)
        total_rows = 12 * 8190  # Full year
        all_data = []
        
        # Generate full year dataset
        for month in range(12):
            month_rows = 8190
            np.random.seed(month)
            
            month_data = pd.DataFrame({
                'timestamp': pd.date_range(f'2024-{month+1:02d}-01', periods=month_rows, freq='1T'),
                'open': 100 + np.random.normal(0, 1, month_rows).cumsum() * 0.01,
                'high': 100 + np.random.normal(0, 1, month_rows).cumsum() * 0.01 + 0.1,
                'low': 100 + np.random.normal(0, 1, month_rows).cumsum() * 0.01 - 0.1,
                'close': 100 + np.random.normal(0, 1, month_rows).cumsum() * 0.01,
                'volume': np.random.randint(1000, 10000, month_rows),
                'rsi': np.random.uniform(20, 80, month_rows),
                'sma_20': 100 + np.random.normal(0, 1, month_rows).cumsum() * 0.01,
                'ema_12': 100 + np.random.normal(0, 1, month_rows).cumsum() * 0.01,
                'volatility': np.random.uniform(0.01, 0.05, month_rows),
                'momentum': np.random.normal(0, 0.02, month_rows),
                'bb_upper': 100 + np.random.normal(0, 1, month_rows).cumsum() * 0.01 + 2
            })
            
            all_data.append(month_data)
        
        # Combine all months (memory intensive)
        print("   üîÑ Concatenating 12 months of data...")
        full_dataset = pd.concat(all_data, ignore_index=True)
        
        after_load_memory = self.process.memory_info().rss / 1024 / 1024
        
        print(f"   üìä Full dataset loaded: {len(full_dataset):,} rows")
        print(f"   üíæ Memory after loading: {after_load_memory:.1f} MB")
        
        # Now process only Q4 data (like the lazy approach)
        q4_start = len(full_dataset) * 3 // 4  # Last 25% of data
        q4_data = full_dataset.iloc[q4_start:]
        
        processed_q4 = self._process_chunk_data(q4_data)
        
        end_time = time.time()
        peak_memory = self.process.memory_info().rss / 1024 / 1024
        total_time = end_time - start_time
        
        # Calculate actual data size
        dataset_size_mb = full_dataset.memory_usage(deep=True).sum() / (1024 * 1024)
        
        print(f"\nüìà TRADITIONAL LOADING RESULTS:")
        print(f"   üìä Full dataset loaded: {dataset_size_mb:.1f} MB (100% of year)")
        print(f"   üéØ Actually used: Q4 data only ({len(q4_data):,} rows)")
        print(f"   üíæ Peak memory: {peak_memory:.1f} MB")
        print(f"   ‚è±Ô∏è Processing time: {total_time:.2f} seconds")
        print(f"   ‚ö†Ô∏è Memory waste: Loaded 12 months, used 3 months")
        
        return {
            'peak_memory_mb': peak_memory,
            'memory_increase_mb': peak_memory - start_memory,
            'data_loaded_mb': dataset_size_mb,
            'processing_time_sec': total_time,
            'data_efficiency_pct': 100.0  # Loaded everything
        }
    
    def _process_chunk_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Simulate realistic data processing (feature engineering, signals, etc.)"""
        
        # Simulate processing: calculate additional indicators and generate trading signals
        processed = data.copy()
        
        # Use available columns for signal generation
        available_cols = processed.columns.tolist()
        
        # Calculate additional features based on available data
        if 'sma_20' in available_cols:
            processed['sma_signal'] = (processed['close'] > processed['sma_20']).astype(int)
        else:
            processed['sma_signal'] = 0
            
        if 'rsi' in available_cols:
            processed['rsi_signal'] = ((processed['rsi'] < 30) | (processed['rsi'] > 70)).astype(int)
        else:
            processed['rsi_signal'] = 0
            
        if 'volatility' in available_cols:
            processed['vol_signal'] = (processed['volatility'] > processed['volatility'].median()).astype(int)
        else:
            processed['vol_signal'] = 0
        
        # Generate trading signals (realistic filtering)
        signals = processed[
            (processed['sma_signal'] == 1) | 
            (processed['rsi_signal'] == 1) | 
            (processed['vol_signal'] == 1)
        ].copy()
        
        # Add signal metadata
        signals['signal_strength'] = np.random.uniform(0.1, 1.0, len(signals))
        signals['confidence'] = np.random.uniform(0.5, 0.95, len(signals))
        
        return signals


def main():
    """Run production scenario benchmark"""
    
    print("üè≠ PRODUCTION SCENARIO: Large Dataset Processing Benchmark")
    print("=" * 70)
    print("Scenario: Processing 1 year of minute-level trading data")
    print("Goal: Analyze Q4 performance (only need last 3 months)")
    print("=" * 70)
    
    benchmark = ProductionScenarioBenchmark()
    
    # Run lazy loading benchmark
    lazy_results = benchmark.simulate_large_lazy_processing()
    
    # Run traditional loading benchmark  
    traditional_results = benchmark.simulate_traditional_processing()
    
    # Calculate the real-world benefits
    memory_savings = ((traditional_results['memory_increase_mb'] - lazy_results['memory_increase_mb']) / 
                     traditional_results['memory_increase_mb']) * 100
    
    data_efficiency_gain = traditional_results['data_loaded_mb'] / lazy_results['data_loaded_mb']
    
    time_savings = ((traditional_results['processing_time_sec'] - lazy_results['processing_time_sec']) / 
                   traditional_results['processing_time_sec']) * 100
    
    print(f"\n" + "=" * 70)
    print("üéØ PRODUCTION SCENARIO RESULTS")
    print("=" * 70)
    
    print(f"üíæ MEMORY EFFICIENCY:")
    print(f"   Lazy Loading Peak Memory: {lazy_results['peak_memory_mb']:.1f} MB")
    print(f"   Traditional Peak Memory: {traditional_results['peak_memory_mb']:.1f} MB")
    print(f"   Memory Savings: {memory_savings:.1f}%")
    
    print(f"\nüìä DATA EFFICIENCY:")
    print(f"   Lazy Loading: {lazy_results['data_loaded_mb']:.1f} MB loaded")
    print(f"   Traditional: {traditional_results['data_loaded_mb']:.1f} MB loaded")
    print(f"   Data Efficiency Gain: {data_efficiency_gain:.1f}x less data loaded")
    
    print(f"\n‚ö° PERFORMANCE:")
    print(f"   Lazy Loading Time: {lazy_results['processing_time_sec']:.2f} seconds")
    print(f"   Traditional Time: {traditional_results['processing_time_sec']:.2f} seconds")
    print(f"   Time Difference: {time_savings:.1f}% {'faster' if time_savings > 0 else 'slower'}")
    
    print(f"\nüéØ KEY INSIGHTS:")
    print(f"   ‚úÖ Selective Loading: Only {lazy_results['data_efficiency_pct']:.0f}% of dataset needed")
    print(f"   ‚úÖ Memory Efficiency: {memory_savings:.1f}% less memory usage")
    print(f"   ‚úÖ Storage Efficiency: {data_efficiency_gain:.1f}x less I/O operations")
    print(f"   ‚úÖ Scalability: Benefits increase with dataset size")
    
    # Generate recommendation
    if memory_savings > 30:
        recommendation = "üöÄ HIGHLY RECOMMENDED: Significant production benefits"
    elif memory_savings > 15:
        recommendation = "‚úÖ RECOMMENDED: Clear memory and efficiency gains"
    elif memory_savings > 5:
        recommendation = "üî∂ CONSIDER: Moderate benefits, good for large datasets"
    else:
        recommendation = "‚ö†Ô∏è LIMITED BENEFIT: Traditional loading may be sufficient"
    
    print(f"\nüè≠ PRODUCTION RECOMMENDATION: {recommendation}")
    
    # Save detailed results
    results = {
        'scenario': 'production_large_dataset',
        'benchmark_timestamp': time.time(),
        'lazy_loading': lazy_results,
        'traditional_loading': traditional_results,
        'comparison': {
            'memory_savings_pct': memory_savings,
            'data_efficiency_gain': data_efficiency_gain,
            'time_savings_pct': time_savings,
            'recommendation': recommendation
        },
        'use_case': {
            'description': 'Year-long dataset, analyze Q4 only',
            'total_dataset_size': '~180 MB (12 months)',
            'actually_needed': '~45 MB (3 months)',
            'efficiency_ratio': '4:1 (load 25%, discard 75%)'
        }
    }
    
    results_path = Path("benchmarks") / "production_lazy_loading_benchmark.json"
    results_path.parent.mkdir(exist_ok=True)
    
    import json
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nüìÑ Detailed results saved to: {results_path}")
    print("üéØ Empirical evidence ready for production deployment decision")


if __name__ == "__main__":
    main()